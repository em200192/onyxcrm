import streamlit as st
from PIL import Image
import os
import io
import fitz  # PyMuPDF
import json
import re
from pathlib import Path
import base64
import torch
from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer, util
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# >>> NEW: std libs for GCS export
import csv
from io import StringIO
from datetime import datetime, timezone
from uuid import uuid4

# --- CONFIGURATION ---
load_dotenv()
st.set_page_config(
    page_title="Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- CONSTANTS & DIRECTORIES ---

# --- NEW: Dual KBs (GL user guide + Errors) ---
BASE_DIR = Path(__file__).parent
UPLOAD_DIR = BASE_DIR / "uploaded_files"
CACHE_DIR = BASE_DIR / "cache"
IMAGE_CACHE_DIR = CACHE_DIR / "solution_images"
GUIDE_PDF_DEFAULT = str((BASE_DIR / "data" / "user_guides" / "Gl.pdf").resolve())
ERRORS_PDF_DEFAULT = str((BASE_DIR / "uploaded_files" / "erros and solution.pdf").resolve())

GUIDE_KB_PATH = CACHE_DIR / "gl_guide_kb.json"
ERRORS_KB_PATH = CACHE_DIR / "gl_errors_kb.json"
GUIDE_IMG_DIR = CACHE_DIR / "guide_images"
os.makedirs(GUIDE_IMG_DIR, exist_ok=True)






from dataclasses import dataclass

# --- Arabic normalization ---
# --- Arabic normalization & helpers ---
AR_NORM_MAP = {'Ø£':'Ø§','Ø¥':'Ø§','Ø¢':'Ø§','Ù‰':'ÙŠ','Ø©':'Ù‡','Ø¤':'Ùˆ','Ø¦':'ÙŠ','Ù°':'','Ù€':''}
AR_DIACRITICS = r'[\u064B-\u065F\u0670]'

def ar_norm(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(AR_DIACRITICS, '', s)
    return ''.join(AR_NORM_MAP.get(ch, ch) for ch in s)

def contains(text: str, *words) -> bool:
    t = ar_norm(text)
    return any(ar_norm(w) in t for w in words)

# Domain synonym buckets (normalized forms)
SYN = {
    "create": {"Ø§Ø¶Ø§Ù","Ø§Ø¶Ø§ÙÙ‡","Ø§Ù†Ø´Ø§Ø¡","Ø§Ø¯Ø®Ø§Ù„","ØªØ³Ø¬ÙŠÙ„","ØªØ¹Ø±ÙŠÙ"},
    "report": {"ØªÙ‚Ø±ÙŠØ±","ØªÙ‚Ø§Ø±ÙŠØ±"},
    "request": {"Ø·Ù„Ø¨","Ø·Ù„Ø¨Ø§Øª"},
    "voucher_word": {"Ø³Ù†Ø¯","Ø³Ù†Ø¯Ø§Øª"},
    "payment": {"ØµØ±Ù","Ø³Ù†Ø¯Ø³Ø±Ù","Ø³Ù†Ø¯Ø§Ù„ØµØ±Ù","Ø³Ù†Ø¯Ø§ØªØ§Ù„ØµØ±Ù"},
    "receipt": {"Ù‚Ø¨Ø¶","Ø³Ù†Ø¯Ù‚Ø¨Ø¶","Ø³Ù†Ø¯Ø§ØªØ§Ù„Ù‚Ø¨Ø¶"},
    "currency": {"Ø¹Ù…Ù„Ù‡","Ø¹Ù…Ù„Ø©","ØµØ±Ù Ø¹Ù…Ù„Ù‡","ØµØ±Ù Ø¹Ù…Ù„Ø©","Ø´Ø±Ø§Ø¡ Ø¹Ù…Ù„Ø©","Ø¨ÙŠØ¹ Ø¹Ù…Ù„Ø©"},
    "supplier": {"Ù…ÙˆØ±Ø¯","Ù…ÙˆØ±Ø¯ÙŠÙ†"},
    "customer": {"Ø¹Ù…ÙŠÙ„","Ø¹Ù…Ù„Ø§Ø¡"},
    "bank": {"Ø¨Ù†Ùƒ","Ù…ØµØ±Ù","Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨Ù†ÙƒÙŠ","Ø­Ø³Ø§Ø¨ Ø¨Ù†ÙƒÙŠ","Ø­Ø³Ø§Ø¨ Ø¨Ù†Ùƒ"},
    "cash_fund": {"ØµÙ†Ø¯ÙˆÙ‚","Ø§Ù„Ø®Ø²Ù†Ø©","Ø®Ø²Ù†Ø©","cash fund"},
    "checkbook": {"Ø¯ÙØªØ± Ø´ÙŠÙƒØ§Øª","Ø¯ÙØªØ± Ø§Ù„Ø´ÙŠÙƒØ§Øª","Ø´ÙŠÙƒ","Ø´ÙŠÙƒØ§Øª","checkbook"}
}


def has_any(text: str, terms: set[str]) -> bool:
    t = ar_norm(text)
    return any(ar_norm(w) in t for w in terms)




@dataclass
class Slots:
    doc_type: str|None = None      # "screen" | "report" | None
    voucher_type: str|None = None  # "receipt" | "payment" | None
    object_key: str|None = None    # "bank" | "cash_fund" | "checkbook" | ...
    action: str|None = None        # "create" | None

def extract_slots(query: str) -> dict:
    q = ar_norm(query)
    slots = {
        "doc_type": None,          # "screen" | "report"
        "voucher_family": None,    # "payment" | "receipt"
        "party": None,             # "supplier" | "customer"
        "wants_request": False,    # user said "Ø·Ù„Ø¨"
        "wants_voucher": False,    # user said "Ø³Ù†Ø¯"
        "money_exchange": False,   # mentions Ø¹Ù…Ù„Ø©
        "action_create": False,    # add/create
    }

    if contains(q, *SYN["report"]):  slots["doc_type"] = "report"
    if contains(q, *SYN["create"]):  slots["action_create"] = True

    if contains(q, *SYN["voucher_word"]): slots["wants_voucher"] = True
    if contains(q, *SYN["request"]):      slots["wants_request"] = True

    if contains(q, *SYN["payment"]): slots["voucher_family"] = "payment"
    if contains(q, *SYN["receipt"]): slots["voucher_family"] = slots["voucher_family"] or "receipt"

    if contains(q, *SYN["currency"]): slots["money_exchange"] = True

    if contains(q, *SYN["supplier"]): slots["party"] = "supplier"
    if contains(q, *SYN["customer"]): slots["party"] = slots["party"] or "customer"

    # If user asked "ÙƒÙŠÙ/Ø§Ø¶Ø§ÙØ©" with no "ØªÙ‚Ø±ÙŠØ±", prefer screens
    if slots["doc_type"] is None and (slots["action_create"] or "ÙƒÙŠÙ" in q):
        slots["doc_type"] = "screen"

    return slots




KB_PATH = CACHE_DIR / "knowledge_base.json"
CONFIDENCE_THRESHOLD = 0.55
SUGGESTION_THRESHOLD = 0.40
MAX_FOLLOW_UP_ATTEMPTS = 2


# >>> NEW: GCS config (optional)
GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")              # e.g. "crm-escalations"
GCS_CREDENTIALS_JSON = os.getenv("GCS_CREDENTIALS_JSON","")    # optional path to SA JSON

# Place near other constants
ACTIVE_INTENT_THRESHOLDS = {
    "close_to_current": 0.40,   # similar to current issue
    "new_issue_candidate": 0.45,# similar to some other KB entry
    "margin": 0.08              # current similarity must exceed next-best by this margin
}

# --- Escalation intent: quick patterns (Arabic + EN) ---
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(IMAGE_CACHE_DIR, exist_ok=True)

# --- CORE AI & KNOWLEDGE BASE FUNCTIONS ---

# --- Escalation intent: quick patterns (Arabic + EN) ---
ESCALATE_PATTERNS = [
    "ÙˆØµÙ„Ù†ÙŠ Ø¨Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±", "ÙˆØµÙ„Ù†ÙŠ Ø¨Ù…Ø³ØªØ´Ø§Ø±", "ÙˆØµÙ„Ù†ÙŠ Ø¨Ø§Ù„Ø¯Ø¹Ù…", "Ø§ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¯Ø¹Ù…",
    "Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¯Ø¹Ù…", "Ø§Ø±ÙŠØ¯ Ø§Ù„Ø¯Ø¹Ù…", "Ø§Ø±ÙŠØ¯ Ø§ØªÙˆØ§ØµÙ„ Ù…Ø¹ ÙØ±ÙŠÙ‚ Ø§Ù„Ø¯Ø¹Ù…", "ÙƒÙ„Ù… Ø§Ù„Ø¯Ø¹Ù…",
    "Ø§ÙƒÙ„Ù… Ø§Ù„Ø¯Ø¹Ù…", "Ù…Ø³ØªØ´Ø§Ø±", "Ù…ÙˆØ¸Ù", "Ø¨Ø´Ø±ÙŠ", "Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¯Ø¹Ù…", "Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹Ù…",
    "Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¯Ø¹Ù…", "Ø­ÙˆÙ„Ù†ÙŠ Ù„Ù„Ø¯Ø¹Ù…", "Ø§Ø±ÙŠØ¯ Ø´Ø®Øµ", "Ø§Ø±ÙŠØ¯ Ù…Ø³Ø¤ÙˆÙ„", "Ø§Ø±ÙŠØ¯ Ù…ÙˆØ¸Ù",
    "talk to human", "talk to agent", "human support", "connect me to support",
    "transfer to agent", "escalate", "escalation", "support team", "helpdesk"
]

# --- SMART FOLLOW-UP CONSTANTS ---
EXPLICIT_ESCALATE_PATTERNS = [
    "Ù…Ø³ØªØ´Ø§Ø±", "Ø§Ù„Ø¯Ø¹Ù…", "support", "help desk", "Ø§Ø±ÙŠØ¯ Ø§ØªÙˆØ§ØµÙ„", "Ø§Ø±ÙŠØ¯ Ø§Ù„ØªÙˆØ§ØµÙ„",
    "Ø§ØªØµÙ„ÙˆØ§ Ø¨ÙŠ", "ÙƒÙ„Ù…Ù†ÙŠ", "Ø§Ø¨ØºÙ‰ Ø§ÙƒÙ„Ù…", "ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„ÙØ±ÙŠÙ‚", "Ø§Ø±ÙŠØ¯ Ø§Ø­Ø¯ ÙŠØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ"
]
NEGATIVE_CUES = [
    "Ù„Ù… Ø£Ø³ØªØ·Ø¹","Ù…Ø§ Ù‚Ø¯Ø±Øª","Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹","Ù…Ø­Ø¬ÙˆØ¨","Ù„Ø§ ØªØ¸Ù‡Ø± Ø§Ù„Ø´Ø§Ø´Ø©","Ø±ÙØ¶","Ø®Ø·Ø£",
    "Ù†ÙØ³ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©","Ù„Ù… ÙŠØ¹Ù…Ù„","Ù„Ø§ ÙŠØ¹Ù…Ù„","Ù…Ø§ Ù†ÙØ¹","Ù…Ø§ Ø²Ø§Ù„Øª","Ù„Ø§ ÙŠØ²Ø§Ù„"
]


# --- Fuzzy helpers for safer suggestions ---



# --- NEW: source router for initial queries ---
ERROR_LIKE_PAT = re.compile(
    r"(?:\bmsg\b|\berror\b|\bmessage\b|Ø±Ø³Ø§Ù„Ø©|Ù…Ø´ÙƒÙ„Ø©)\s*(?:no|Ø±Ù‚Ù…)?\s*\d{2,}|"
    r"(?:\b\d{3,}\b\s*(?:msg|error|message))|"
    r"(?:Ø§Ù„Ø±Ø³Ø§Ù„Ø©\s*Ø±Ù‚Ù…\s*\d{2,})",
    re.IGNORECASE
)

# Add this pattern (3+ digits with only whitespace around)
DIGITS_ONLY_PAT = re.compile(r"^\s*\d{3,}\s*$")

# Accept Arabic or English plain 3+ digits
DIGITS_ONLY_PAT = re.compile(r"^\s*(?:\d|[Ù -Ù©]){3,}\s*$")



@st.cache_resource
def create_competition_router_chain():
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""
Ø£Ù†Øª Ù…Ø­ÙƒÙ‘ÙÙ… Ù…ØµØ§Ø¯Ø±. Ù„Ø¯ÙŠÙƒ Ø³Ø¤Ø§Ù„ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ùˆ"Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†" Ù…Ù† Ù…ØµØ¯Ø±ÙŠÙ†:
- Errors: Ù…Ø´Ø§ÙƒÙ„ ÙØ¹Ù„ÙŠØ© Ø¨Ø±Ø³Ø§Ø¦Ù„ ÙˆØ£Ø±Ù‚Ø§Ù… ÙˆØ­Ù„ÙˆÙ„
- Guide: Ø´Ø§Ø´Ø§Øª/Ø®Ø·ÙˆØ§Øª/Ø´Ø±ÙˆØ­Ø§Øª

Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª:
- Ø§Ù„Ø³Ø¤Ø§Ù„: {query}
- Ù…Ø±Ø´Ø­Ùˆ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ (id | score | message_number | message_text | reason):
{errors_block}

- Ù…Ø±Ø´Ø­Ùˆ Ø§Ù„Ø¯Ù„ÙŠÙ„ (id | score | title | category | snippet):
{guide_block}

Ù‚Ø±Ù‘ÙØ± Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ø­Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¢Ù†ØŒ Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø°Ø§ØªÙ‡Ø§ Ù„Ø§ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©.
Ø£Ø¬Ø¨ JSON ÙÙ‚Ø·:
{{
  "source": "errors" | "guide",
  "confidence": 0.0_to_1.0,
  "reason": "<Ù…Ù„Ø®Øµ Ù‚ØµÙŠØ±>"
}}
""",
        input_variables=["query", "errors_block", "guide_block"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)


def _topk_indices(scores: torch.Tensor, k: int):
    k = min(k, scores.shape[-1])
    vals, idxs = torch.topk(scores, k)
    return idxs.tolist(), vals.tolist()

def _build_errors_block(kb, idxs, vals):
    lines = []
    for i, s in zip(idxs, vals):
        it = kb[i]
        num = str(it.get("message_number","")).strip()
        msg = (it.get("message_text") or "").replace("\n"," ")[:140]
        rsn = (it.get("reason") or "").replace("\n"," ")[:120]
        lines.append(f"{i} | {s:.3f} | {num} | {msg} | {rsn}")
    return "\n".join(lines)

def _build_guide_block(kb, idxs, vals):
    lines = []
    for i, s in zip(idxs, vals):
        it = kb[i]
        ttl = (it.get("title") or "").replace("\n"," ")[:140]
        cat = (it.get("category") or "").strip()
        snp = (it.get("body") or "").replace("\n"," ")[:160]
        lines.append(f"{i} | {s:.3f} | {ttl} | {cat} | {snp}")
    return "\n".join(lines)


# ---- Smart error-id extraction ----
ERROR_CONTEXT_WORDS = {
    "Ø±Ø³Ø§Ù„Ù‡","Ø±Ø³Ø§Ù„Ø©","Ø®Ø·Ø§","Ø®Ø·Ø£","ÙƒÙˆØ¯","Ø±Ù…Ø²","Ø±Ù‚Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø©","Ø±Ù‚Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø©","Ø§Ù„Ø±Ø³Ø§Ù„Ø©",
    "message","msg","error","code","problem","issue"
}
CURRENCY_WORDS = {"sar","Ø±ÙŠØ§Ù„","Ø¯Ø±Ù‡Ù…","usd","egp","Ø¬Ù†ÙŠÙ‡","eur","$","Ùª","%","Ø±.Ø³","Ø¯.Ø¥"}
DATE_PAT = re.compile(r"\b(?:19|20)\d{2}\b")   # 1900â€“2099 (year-like)
TIME_PAT = re.compile(r"\b\d{1,2}[:Ù«.]\d{2}\b")# e.g., 12:30 or 12Ù«30

@st.cache_resource
def create_error_id_extractor_chain():
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""
Ø§Ø³ØªØ®Ø±Ø¬ Ø£Ø±Ù‚Ø§Ù… Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ù‚ØµÙˆØ¯Ø© Ù…Ù† Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø¥Ù† ÙˆÙØ¬Ø¯Øª).
- ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„Ù…Ø¨Ø§Ù„Øº ÙˆØ§Ù„Ø£ÙˆÙ‚Ø§Øª ÙˆØ§Ù„ØªÙˆØ§Ø±ÙŠØ® ÙˆØ£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‡ÙˆØ§ØªÙ.
- Ø§Ø¹ØªØ¨Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª: Ø±Ø³Ø§Ù„Ø©ØŒ Ø®Ø·Ø£ØŒ ÙƒÙˆØ¯ØŒ Ø±Ù…Ø²ØŒ Ø±Ù‚Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø©ØŒ message, error, code, msg, problem, issue Ù…Ø¤Ø´Ø±Ø§Øª Ù‚ÙˆÙŠØ©.
Ø£Ø¬Ø¨ JSON ÙÙ‚Ø·:
{"error_ids": ["..."], "confidence": 0.0_to_1.0}

Ø§Ù„Ù†Øµ:
{text}
""",
        input_variables=["text"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

def extract_error_ids_smart(text: str) -> dict:
    s = to_english_digits((text or "").strip())
    s_low = s.lower()

    # candidates: 3â€“6 digits (covers 3, 4, 5, 6)
    matches = list(re.finditer(r"(?<!\d)(\d{3,6})(?!\d)", s))
    if not matches:
        return {"ids": [], "confidence": 0.0}

    valid = st.session_state.get("valid_message_numbers", set()) or set()

    def is_noise(m):
        n = m.group(1)
        win = s_low[max(0, m.start()-15): m.end()+15]
        if TIME_PAT.search(win): return True
        if DATE_PAT.search(win) and len(n) == 4: return True
        if any(w in win for w in CURRENCY_WORDS): return True
        # simple phone-ish: 7â€“10+ digits in full input (we're only scoring 3â€“6 but keep guard)
        if re.search(r"\b\d{7,}\b", s): return True
        return False

    scored = []
    for m in matches:
        n = m.group(1)
        if is_noise(m):
            continue
        score = 0.35
        # known KB number = strongest
        if n in valid:
            score = 1.0
        # context words near number
        ctx = s_low[max(0, m.start()-12): m.end()+12]
        if any(w in ctx for w in ERROR_CONTEXT_WORDS):
            score += 0.4
        # bare-number query (just the number) is very strong
        if s.strip() == n:
            score += 0.5
        scored.append((n, min(score, 1.0)))

    scored.sort(key=lambda x: x[1], reverse=True)
    ids = [n for n, _ in scored]
    conf = scored[0][1] if scored else 0.0

    # LLM tie-breaker if still unsure
    if conf < 0.6:
        try:
            raw = create_error_id_extractor_chain().invoke({"text": text})["text"]
            data = _json_only(raw) or {}
            llm_ids = [to_english_digits(str(x)) for x in (data.get("error_ids") or [])]
            # merge (preserve order/uniques)
            ids = list(dict.fromkeys(llm_ids + ids))
            conf = max(conf, float(data.get("confidence", 0.0)))
        except Exception:
            pass

    return {"ids": ids, "confidence": conf}




def route_by_competition(query: str, k: int = 6, margin: float = 0.03) -> dict:
    """
    Pure retrieval competition:
    1) Embed query
    2) Take top-k from errors + top-k from guide
    3) If best score differs by a clear margin -> pick that source
    4) Else ask LLM arbiter (no keyword rules)
    Returns: {"source": "errors"|"guide", "confidence": float,
              "best_error_idx": int|None, "best_guide_idx": int|None}
    """
    _ensure_kb_matrices()

    q_norm = to_english_digits((query or "").strip())
    mentioned_nums = re.findall(r"\b\d{3,}\b", q_norm)
    valid_nums = st.session_state.get("valid_message_numbers", set()) or set()

    if mentioned_nums:
        # If *any* mentioned number exists in the errors KB â†’ force errors
        if any(n in valid_nums for n in mentioned_nums):
            return {"source": "errors", "confidence": 0.99, "best_error_idx": None, "best_guide_idx": None}
        # If user gave a number but it's not in KB, still route to errors
        # so your existing logic can escalate gracefully.
        return {"source": "errors", "confidence": 0.75, "best_error_idx": None, "best_guide_idx": None}

    errors_kb = st.session_state.get("gl_errors_kb") or []
    guide_kb  = st.session_state.get("gl_guide_kb") or []
    if not errors_kb and not guide_kb:
        return {"source":"guide","confidence":0.0,"best_error_idx":None,"best_guide_idx":None}

    model = get_embedding_model()
    qemb = model.encode((query or "").strip(), convert_to_tensor=True)

    # scores
    best_err = best_gui = None
    err_idxs = err_vals = gui_idxs = gui_vals = []
    if errors_kb and "gl_errors_matrix" in st.session_state:
        s_err = util.cos_sim(qemb, st.session_state.gl_errors_matrix)[0]
        err_idxs, err_vals = _topk_indices(s_err, k)
        best_err = (err_idxs[0], float(err_vals[0])) if err_idxs else (None, 0.0)
    else:
        best_err = (None, 0.0)

    if guide_kb and "gl_guide_matrix" in st.session_state:
        s_gui = util.cos_sim(qemb, st.session_state.gl_guide_matrix)[0]
        gui_idxs, gui_vals = _topk_indices(s_gui, k)
        best_gui = (gui_idxs[0], float(gui_vals[0])) if gui_idxs else (None, 0.0)
    else:
        best_gui = (None, 0.0)

    # 1) clear winner by margin
    be_i, be_s = best_err
    bg_i, bg_s = best_gui
    if be_i is not None and (be_s >= bg_s + margin):
        return {"source":"errors","confidence":min(1.0, be_s), "best_error_idx":be_i, "best_guide_idx":bg_i}
    if bg_i is not None and (bg_s >= be_s + margin):
        return {"source":"guide","confidence":min(1.0, bg_s), "best_error_idx":be_i, "best_guide_idx":bg_i}

    # 2) too close â†’ ask LLM arbiter on the actual top-k texts
    arb = create_competition_router_chain()
    errors_block = _build_errors_block(errors_kb, err_idxs, err_vals) if err_idxs else "(no candidates)"
    guide_block  = _build_guide_block (guide_kb,  gui_idxs,  gui_vals)  if gui_idxs  else "(no candidates)"
    try:
        raw = arb.invoke({
            "query": query,
            "errors_block": errors_block,
            "guide_block": guide_block
        })["text"]
        data = _json_only(raw) or {}
        src = data.get("source", "guide")
        conf = float(data.get("confidence", 0.55))
        return {"source": src, "confidence": conf, "best_error_idx": be_i, "best_guide_idx": bg_i}
    except Exception:
        # fallback to simple tie-break: pick the higher raw score even if close
        if (be_s >= bg_s):  # deterministic
            return {"source":"errors","confidence":min(1.0, be_s), "best_error_idx":be_i, "best_guide_idx":bg_i}
        else:
            return {"source":"guide","confidence":min(1.0, bg_s), "best_error_idx":be_i, "best_guide_idx":bg_i}

# --- Fallback JSON extractor (use if you don't already have one) ---
def _json_only(s: str) -> dict:
    m = re.search(r'\{.*\}', s, re.DOTALL)
    return json.loads(m.group(0)) if m else {}



@st.cache_resource
def create_tip_generation_chain():
    """
    Creates an LLM chain that reads a block of text and generates a single,
    engaging 'Did you know?' tip from it.
    """
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""You are a helpful assistant who creates 'Did you know?' tips from technical ERP software manuals.
Read the following topic content. Your task is to find or generate one single, interesting, and useful tip for a user.

- The tip should be a complete sentence and written in engaging Arabic.
- It should be a practical piece of advice, a shortcut, or an interesting fact.
- Do NOT simply repeat a "Usage" or "Purpose" sentence. Find a real insight.
- If you cannot find a good, interesting tip in the text, you MUST respond with an empty JSON object.

TOPIC CONTENT:
---
{content}
---

Respond with a valid JSON object only, like this:
{{"tip": "<your generated tip>"}}
""",
        input_variables=["content"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

@st.cache_resource
def create_followup_router_chain():
    """
    Decides if a follow-up question is about the error solution itself
    or a general "how-to" query that requires searching the user guide.
    """
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""You are an intent classifier. A user is currently solving an error. Your task is to classify their follow-up question.

The error's solution involves these steps: {solution_context}

The user's follow-up question is: "{query}"

Does the user's question ask for more detail about the specific error's solution, or is it a more general "how-to" question about a feature (like a screen or a process) mentioned in the solution?

Choose one of the following intents:
- "troubleshoot_error": If the user is asking directly about the error or its solution steps.
- "search_guide": If the user is asking a general "how-to" question (e.g., "how do I open that screen?", "what is a ...?").

Respond with a valid JSON object only, like this:
{{"intent": "<troubleshoot_error|search_guide>"}}
""",
        input_variables=["solution_context", "query"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)


# --- LLM source router: "errors" vs "guide" ---
@st.cache_resource
def create_source_router_chain():
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""
ØµÙ†Ù‘Ù Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ Ù…ØµØ¯Ø± ÙˆØ§Ø­Ø¯:
- "errors" Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø´ÙƒÙ„Ø© Ø¨Ø±Ø³Ø§Ù„Ø©/ÙƒÙˆØ¯ØŒ Ø®ØµÙˆØµÙ‹Ø§ Ù…Ø¹ Ø±Ù‚Ù… 3+ Ø£Ø±Ù‚Ø§Ù….
- "guide" Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø´Ø§Øª/Ø§Ù„Ø®Ø·ÙˆØ§Øª/Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± (ÙƒÙŠÙ/Ø¥Ø¶Ø§ÙØ©/ØªØ¹Ø±ÙŠÙ/ØªÙ‚Ø±ÙŠØ±).

Ø§Ù„Ù†Øµ: {q}

JSON ÙÙ‚Ø·:
{{"source":"errors"|"guide"}}
""",
        input_variables=["q"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)



def decide_source_smart(q: str) -> str:
    raw = (q or "").strip()

    # (A) Smart number-first detector (3â€“6 digits, Arabic/English, context-aware)
    det = extract_error_ids_smart(raw)
    if det["ids"] and det["confidence"] >= 0.6:
        st.session_state["__detected_error_ids__"] = det["ids"]
        return "errors"

    # (B) Strong how-to/report cues â†’ guide
    if any(c in raw for c in ["ÙƒÙŠÙ","Ø·Ø±ÙŠÙ‚Ø©","Ø§Ø¶Ø§Ù","Ø¥Ø¶Ø§ÙØ©","ØªØ¹Ø±ÙŠÙ","Ø´Ø§Ø´Ø©","ØªÙ‚Ø±ÙŠØ±","report"]):
        return "guide"

    # (C) Fallback: quick patterns
    raw_norm = to_english_digits(raw.lower())
    if re.search(r"(Ø±Ø³Ø§Ù„Ø©|Ø®Ø·[Ø§Ø£]Ø¡|problem|error|code|message|msg)\s*\d{3,6}", raw_norm):
        return "errors"

    # (D) LLM tie-breaker
    try:
        r = create_source_router_chain().invoke({"q": q})["text"]
        return (_json_only(r) or {}).get("source", "guide")
    except Exception:
        return "guide"

from difflib import SequenceMatcher

AR_LETTERS = r"\u0600-\u06FF"

def _arabic_english_normalize(s: str) -> str:
    s = to_english_digits((s or "").lower())
    # keep Arabic letters, ASCII letters/digits, spaces
    s = re.sub(fr"[^{AR_LETTERS}a-z0-9\s]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _lexical_ratio(a: str, b: str) -> float:
    """Simple, fast similarity (0..1) based on SequenceMatcher."""
    return SequenceMatcher(None, _arabic_english_normalize(a), _arabic_english_normalize(b)).ratio()

def _token_coverage(query: str, target: str) -> float:
    qt = _arabic_english_normalize(query).split()
    tt = set(_arabic_english_normalize(target).split())
    if not qt:
        return 0.0
    hit = sum(1 for w in qt if w in tt)
    return hit / len(qt)

def _explicit_escalation_requested(text: str) -> bool:
    t = (text or "").lower()
    return any(p in t for p in EXPLICIT_ESCALATE_PATTERNS)

def _stuck_score(user_text: str, last_assistant_reply: str) -> int:
    """Tiny heuristic: +1 if user expresses failure, +1 if assistant repeated itself,
    +1 if no new action words appear."""
    score = 0
    u = (user_text or "")
    a = (last_assistant_reply or "")
    if any(c in u for c in NEGATIVE_CUES): score += 1
    # repeated advice (same reply twice)
    if st.session_state.get("last_assistant_hash") == hash(a.strip()):
        score += 1
    # little novelty: no verbs suggesting an action
    if not re.search(r"(Ø§Ø­ÙØ¸|Ø§ÙØªØ­|Ø§Ø°Ù‡Ø¨|ØªØ£ÙƒØ¯|ÙØ¹Ù‘Ù„|Ø£Ø¹Ø¯|Ø­Ø¯Ù‘Ø«|Ø¬Ø±Ù‘Ø¨|ØºÙŠÙ‘Ø±|Ø£ØºÙ„Ù‚|Ø£Ø¹Ø¯ ÙØªØ­)", a):
        score += 1
    return score




def get_did_you_know_tip(user_query: str | None = None) -> dict | None:
    """
    Returns: {"text": tip, "title": title, "page": page}
    - If user_query is provided: pick the most similar tip (embeddings).
    - Else: rotate a daily random tip (stable per day).
    """
    tips = load_gl_dyk_index()
    if not tips:
        return None

    # Context-aware mode
    if user_query:
        try:
            qemb = get_embedding_model().encode(user_query.strip(), convert_to_tensor=True)
            mat = torch.tensor([t["embedding"] for t in tips])
            sims = util.cos_sim(qemb, mat)[0]
            idx = int(torch.argmax(sims).item())
            best = tips[idx]
            return {"text": best["tip"], "title": best["title"], "page": best["page"]}
        except Exception:
            pass

    # Daily-rotating deterministic pick
    seed = int(datetime.utcnow().strftime("%Y%m%d"))
    idx = seed % len(tips)
    t = tips[idx]
    return {"text": t["tip"], "title": t["title"], "page": t["page"]}



def render_did_you_know_sidebar():
    if build_gl_dyk_index():  # ensures ready
        tip = get_did_you_know_tip(None)
        if tip:
            with st.sidebar:
                st.markdown("### â„¹ï¸ Ù‡Ù„ ØªØ¹Ù„Ù…ØŸ / Did you know?")
                st.info("Ù‡Ù„ ØªØ¹Ù„Ù… Ø§Ù†Ùƒ ØªØ³ØªØ·ÙŠØ¹ Ù…Ø¹Ø±ÙØ© Ù…ØµØ±ÙˆÙØ§Øª ÙˆØ¥ÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¯Ø®Ù„")



# --- NEW: Did-You-Know (DYK) index for GL guide ---
DYK_INDEX_PATH = CACHE_DIR / "gl_guide_dyk.json"


@st.cache_resource
def build_gl_dyk_index(force: bool = False) -> bool:
    """
    Upgraded version: Uses an LLM to generate high-quality, engaging tips
    from each section of the GL guide KB.
    """
    try:
        # We add force=True here to ensure we regenerate with the new logic
        if (not force) and DYK_INDEX_PATH.exists() and not force:
            # If you want to force a rebuild every time, remove this check
            return True

        # Ensure the main guide KB is loaded
        if "gl_guide_kb" not in st.session_state or not st.session_state.gl_guide_kb:
            if not load_gl_guide_kb():
                print("Could not load GL Guide KB to build DYK index.")
                return False

        guide = st.session_state.get("gl_guide_kb") or []
        tip_chain = create_tip_generation_chain()
        all_tips = []

        print("ğŸ§  Starting AI-powered 'Did You Know?' tip generation...")
        with st.spinner("Generating smart tips with AI..."):
            for i, sec in enumerate(guide):
                title = (sec.get("title") or "").strip()
                page = int(sec.get("page") or 0)
                body = (sec.get("body") or "")

                # Skip if there's no body or the page number is invalid
                if not body or page == 0:
                    continue

                print(f"  -> Analyzing topic {i + 1}/{len(guide)}: {title}")
                try:
                    # Ask the LLM to generate a tip from the body text
                    response_text = tip_chain.invoke({"content": body})["text"]
                    tip_data = _json_only(response_text)

                    if tip_data and "tip" in tip_data and tip_data["tip"]:
                        all_tips.append({
                            "tip": tip_data["tip"],
                            "title": title[:160],
                            "page": page,
                        })
                except Exception as e:
                    # This can happen if the LLM returns an empty response or fails
                    print(f"    -> No tip generated for '{title}'. Skipping.")
                    continue

        if not all_tips:
            print("No tips were generated by the AI.")
            return False

        # Embed the new, high-quality tips
        print("Embedding new tips...")
        model = get_embedding_model()
        embs = model.encode([c["tip"] for c in all_tips], convert_to_tensor=True)
        for i, c in enumerate(all_tips):
            c["embedding"] = embs[i].tolist()

        with open(DYK_INDEX_PATH, "w", encoding="utf-8") as f:
            json.dump(all_tips, f, ensure_ascii=False, indent=2)
        print(f"Successfully generated and saved {len(all_tips)} smart tips.")
        return True
    except Exception as e:
        st.warning(f"An error occurred while building the 'Did You Know?' index: {e}")
        return False

def load_gl_dyk_index() -> list:
    if DYK_INDEX_PATH.exists():
        try:
            with open(DYK_INDEX_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return []
    return []




def generate_guide_instructions(section: dict, user_query: str = "") -> dict:
    """
    Turns a GL guide section into a clean answer with a format
    based on the section's category.
    Returns {"answer": str, "images": [paths...]}.
    """
    llm = get_llm_model()
    title = (section.get("title") or "").strip()
    body  = (section.get("body")  or "").strip()
    category = section.get("category", "Inputs") # Default to 'Inputs' if category is missing

    # --- PROMPT SELECTION LOGIC ---
    prompt = ""

    # 1. Format for System Inputs & Operations (Your existing instructed format)
    if category in ["Inputs", "Operations"]:
        prompt = f"""
Ø£Ù†Øª ÙƒØ§ØªØ¨ Ø£Ø¯Ù„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­ØªØ±Ù. ØµÙØº Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù…Ù„ÙŠØ© Ù…ÙˆØ¬Ù‘Ù‡Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚Ø³Ù… Ù…Ù† Ø¯Ù„ÙŠÙ„ Ù…Ø³ØªØ®Ø¯Ù… Onyx ERP (GL).

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: "{user_query}"

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
- Ø§ØªØ¨Ø¹ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø¨Ø¯Ù‚Ø©.
- Ù„Ø®Ù‘Øµ ÙˆØ£Ø¹Ø¯ Ø§Ù„ØµÙŠØ§ØºØ© Ù„ØªØµØ¨Ø­ Ø®Ø·ÙˆØ§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ°.

### ğŸ“˜ {title}
**ğŸ§­ Ø§Ù„Ù‡Ø¯Ù:** Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ ÙŠÙˆØ¶Ø­ Ù…Ø§ Ø§Ù„Ø°ÙŠ Ø³ÙŠÙ†Ø¬Ø²Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
### ğŸ› ï¸ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
(Ø­ÙˆÙ‘Ù„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø®Ø·ÙˆØ§Øª Ù…Ø±Ù‚Ù…Ø© ÙˆÙˆØ§Ø¶Ø­Ø©: 1. ... 2. ... 3. ...)
### ğŸ’¡ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù‡Ø§Ù…Ø©
(Ù„Ø®Ù‘Øµ Ø£ÙŠ Ù†Ù‚Ø§Ø· Ù…Ù‡Ù…Ø© Ù‡Ù†Ø§ØŒ Ø¥Ù† ÙˆØ¬Ø¯Øª)

Ù†Øµ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ù„ØªØ­ÙˆÙŠÙ„Ù‡:
\"\"\"{body[:3500]}\"\"\"
""".strip()

    # 2. Format for System Configuration (New "System Usage Configuration" format)
    elif category == "Configuration":
        prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ù†Ø¸Ø§Ù… Onyx ERP. Ø§Ø´Ø±Ø­ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙˆØ¬Ø²Ø©.

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: "{user_query}"
Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ØªÙƒÙˆÙŠÙ†: "{title}"

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
- Ø§ØªØ¨Ø¹ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø¨Ø¯Ù‚Ø©.
- Ø±ÙƒØ² Ø¹Ù„Ù‰ "Ù„Ù…Ø§Ø°Ø§" Ùˆ "Ù…ØªÙ‰" ÙŠØ³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯.

### âš™ï¸ ØªÙ‡ÙŠØ¦Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù…: {title}
**Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯:**
(Ø§Ø´Ø±Ø­ Ù‡Ù†Ø§ Ø¨ÙˆØ¶ÙˆØ­ Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØªØ£Ø«ÙŠØ±Ù‡ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù… ÙÙŠ ÙÙ‚Ø±Ø© ÙˆØ§Ø­Ø¯Ø©.)

**Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯:**
(Ù„Ø®Ù‘Øµ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆÙ…Ø§Ø°Ø§ ÙŠØ¹Ù†ÙŠ ÙƒÙ„ Ø®ÙŠØ§Ø± ÙÙŠ Ù†Ù‚Ø§Ø·.)

Ù†Øµ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ù„Ù„Ù…Ù„Ø®Øµ:
\"\"\"{body[:3500]}\"\"\"
""".strip()

    # 3. Format for System Reports (New "Purpose of Use" format)
    elif category == "Reports":
        prompt = f"""
Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø£Ø¹Ù…Ø§Ù„ ØªØ´Ø±Ø­ ÙØ§Ø¦Ø¯Ø© ØªÙ‚Ø±ÙŠØ± Ù…Ø§Ù„ÙŠ Ù…Ù† Ù†Ø¸Ø§Ù… Onyx ERP.

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: "{user_query}"
Ø§Ø³Ù… Ø§Ù„ØªÙ‚Ø±ÙŠØ±: "{title}"

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
- Ø§ØªØ¨Ø¹ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø¨Ø¯Ù‚Ø©.
- Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ù„ØªÙ‚Ø±ÙŠØ±.

### ğŸ“Š Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {title}
**Ù…Ø§ Ù‡Ùˆ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ±ØŸ**
(Ø§Ø´Ø±Ø­ Ø¨ÙˆØ¶ÙˆØ­ Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ¹Ø±Ø¶Ù‡Ø§ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ ÙÙ‚Ø±Ø© ÙˆØ§Ø­Ø¯Ø©.)

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ùˆ Ù…Ù‡Ù…ØŸ**
(ÙˆØ¶Ø­ Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ³Ø§Ø¹Ø¯ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø¹Ù„Ù‰ Ø§ØªØ®Ø§Ø°Ù‡Ø§ØŒ ÙˆÙ…Ù† Ù‡Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙˆÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠÙˆÙ† Ù„Ù‡.)

Ù†Øµ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ù„Ù„Ù…Ù„Ø®Øµ:
\"\"\"{body[:3500]}\"\"\"
""".strip()

    # --- LLM INVOCATION AND RESPONSE ---
    try:
        resp = llm.invoke(prompt)
        md = (resp.content or "").strip()
    except Exception:
        # Fallback to a simple format if the LLM fails
        md = f"### ğŸ“˜ {title}\n\n{body}"

    imgs = section.get("images") or []
    return {"answer": md, "images": imgs}

@st.cache_resource
def create_escalation_judge_chain():
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""
Ø£Ù†Øª "Ù…ÙÙ‚ÙŠÙ‘ÙÙ… Ù…ØªØ§Ø¨Ø¹Ø©". Ù„Ø¯ÙŠÙƒ:
- Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ø±Ù‚Ù… {message_number} | Ù†Øµ: {message_text}
- Ø§Ù„Ø³Ø¨Ø¨: {reason}
- Ø§Ù„Ø­Ù„ Ø§Ù„ÙƒØ§Ù…Ù„: {solution}
- Ø¢Ø®Ø± Ø±Ø¯ÙŠÙ† Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯: [{a1}] Ø«Ù… [{a2}]
- Ø¢Ø®Ø± Ø±Ø¯ÙŠÙ† Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…: [{u1}] Ø«Ù… [{u2}]
- Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¢Ù†: {user_text}
- Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¢Ù†: {assistant_proposal}

Ù‚ÙŠÙ‘ÙÙ…:
1) Ù‡Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ **Ø¬Ø¯ÙŠØ¯ ÙˆÙ…ÙÙŠØ¯** (ÙˆÙ„ÙŠØ³ ØªÙƒØ±Ø§Ø±Ù‹Ø§ ÙˆØ§Ø¶Ø­Ù‹Ø§)ØŸ
2) Ù‡Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… **Ù…Ø­Ø¬ÙˆØ¨/Ù…Ù‚ÙŠÙ‘Ø¯** (ØµÙ„Ø§Ø­ÙŠØ§Øª/Ø´Ø§Ø´Ø© Ù„Ø§ ØªÙØªØ­/Ø¬Ø±Ù‘Ø¨ ÙˆÙ„Ù… ÙŠÙ†Ø¬Ø­)ØŸ
3) Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø£Ù†Ù†Ø§ Ø¨Ø­Ø§Ø¬Ø© Ù„Ù„ØªØµØ¹ÙŠØ¯ Ø§Ù„Ø¢Ù†.

Ø£Ø¬Ø¨ JSON ÙÙ‚Ø·:
{{
  "new_action_quality": 0.0_to_1.0,
  "blocked_signals": ["..."],
  "escalate_prob": 0.0_to_1.0,
  "short_reason": "<Ø³Ø¨Ø¨ Ù…Ø®ØªØµØ±>"
}}
""",
        input_variables=[
            "message_number","message_text","reason","solution",
            "a1","a2","u1","u2","user_text","assistant_proposal"
        ]
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

@st.cache_resource
def create_escalation_intent_chain():
    llm = get_llm_model()
    prompt = """
Ø£Ù†Øª Ù…ØµÙ†Ù‘ÙÙ Ù†ÙˆØ§ÙŠØ§. Ù„Ø¯ÙŠÙƒ Ø¢Ø®Ø± Ø±Ø³Ø§Ù„ØªÙŠÙ† Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø¥Ù† ÙˆØ¬ÙØ¯ØªØ§) ÙˆØ±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©.
Ø­Ø¯Ù‘Ø¯ Ù‡Ù„ ÙŠØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØµØ±Ø§Ø­Ø©Ù‹ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ "Ù…Ø³ØªØ´Ø§Ø±/Ø¯Ø¹Ù… Ø¨Ø´Ø±ÙŠ" Ø£Ùˆ ÙŠØ±ÙŠØ¯ "ØªØµØ¹ÙŠØ¯" Ø§Ù„Ø¢Ù†.

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø·:
{"escalate": true | false, "confidence": 0.0}

- Ø¢Ø®Ø± Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: [1] {last_user_1}  |  [2] {last_user_2}
- Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: {user_text}
"""
    return LLMChain(
        llm=llm,
        prompt=PromptTemplate(
            template=prompt,
            input_variables=["last_user_1", "last_user_2", "user_text"]
        ),
        verbose=False
    )

def _wants_handoff(user_text: str) -> bool:
    """Generic, dynamic check: (1) fast patterns, (2) LLM intent."""
    t = (user_text or "").strip().lower()
    if any(p in t for p in ESCALATE_PATTERNS):
        return True
    try:
        msgs = _last_msgs(2)  # you already have this helper
        chain = create_escalation_intent_chain()
        raw = chain.invoke({
            "last_user_1": msgs["u1"],
            "last_user_2": msgs["u2"],
            "user_text": user_text
        })["text"]
        data = _json_from_model_text(raw)
        return bool(data.get("escalate")) and float(data.get("confidence", 0.0)) >= 0.5
    except Exception:
        return False

@st.cache_resource
def create_blocked_detector_chain():
    llm = get_llm_model()
    prompt = """
Ø£Ù†Øª Ù…ØµÙ†Ù‘ÙÙ Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ø«Ù‘Ø±/Ø§Ù„Ø§Ù†Ø³Ø¯Ø§Ø¯ Ù„Ø¯Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© ØªØ¯Ù„ Ø£Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… **ØºÙŠØ± Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©** (Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹/Ù„Ø§ Ø£ØµÙ„/Ù„Ø§ Ø£Ù…Ù„Ùƒ ØµÙ„Ø§Ø­ÙŠØ©/Ø¬Ø±Ù‘Ø¨Øª ÙˆÙ„Ù… ÙŠÙ†Ø¬Ø­/...),
ÙØ£Ø¬Ø¨ Ø¨Ø£Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… "blocked".

JSON ÙÙ‚Ø·:
{"blocked": true | false, "confidence": 0.0}

- Ø¢Ø®Ø± Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: [1] {last_user_1}  |  [2] {last_user_2}
- Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: {user_text}
"""
    return LLMChain(
        llm=llm,
        prompt=PromptTemplate(
            template=prompt,
            input_variables=["last_user_1", "last_user_2", "user_text"]
        ),
        verbose=False
    )

@st.cache_resource
def get_llm_model():
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Google API. ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØªÙ‡ Ø¥Ù„Ù‰ Ù…Ù„Ù .env")
        st.stop()
    return ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=api_key, temperature=0.1)

@st.cache_resource
def get_embedding_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

def _last_assistant_message_text() -> str:
    for msg in reversed(st.session_state.chat_history):
        if msg["role"] == "assistant":
            return msg.get("content", "")[:800]
    return ""

def _numbers_catalog(max_chars: int = 6000) -> str:
    nums = sorted(list(st.session_state.get("valid_message_numbers", set())))
    s = ", ".join(nums)
    return s[:max_chars]

def _json_from_model_text(text: str):
    # robust JSON extraction (code fence tolerant)
    m = re.search(r'\{.*\}', text, re.DOTALL)
    if not m:
        raise ValueError("No JSON object found in model output")
    return json.loads(m.group(0))

@st.cache_resource
@st.cache_resource
def create_intent_router_chain():
    llm = get_llm_model()
    router_prompt = """
Ø£Ù†Øª ÙˆÙƒÙŠÙ„ ØªØµÙ†ÙŠÙ Ø°ÙƒÙŠ Ù„Ø¯Ø¹Ù… Ù†Ø¸Ø§Ù… Onyx ERP. Ù„Ø¯ÙŠÙƒ Ù…Ø´ÙƒÙ„Ø© Ù†Ø´Ø·Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§ ÙˆØ±Ø¨Ù…Ø§ ÙŠØ°ÙƒØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø±Ù‚Ù… Ø±Ø³Ø§Ù„Ø© Ù…Ø®ØªÙ„ÙØŒ
ÙˆØ±Ø¨Ù…Ø§ ÙŠØ·Ù„Ø¨ ÙØªØ­ Ù…Ø´ÙƒÙ„Ø©/ØªØ°ÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©.

Ù…Ù‡Ù…ØªÙƒ: ØªØ­Ø¯ÙŠØ¯ Ù†ÙŠØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø¯Ù‚Ø©ØŒ ÙˆØ¥Ø±Ø¬Ø§Ø¹ JSON ÙÙ‚Ø· ÙˆÙÙ‚ Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯.

Ø§Ù„Ø³ÙŠØ§Ù‚:
- Ø±Ù‚Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù†Ø´Ø·Ø©: {current_number}
- Ù†Øµ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù†Ø´Ø·Ø© (Ù…Ø®ØªØµØ±): {current_text}
- Ø§Ù„Ø³Ø¨Ø¨ (Ù…Ø®ØªØµØ±): {current_reason}
- Ø§Ù„Ø­Ù„ (Ù…Ø®ØªØµØ±): {current_solution}
- Ø¢Ø®Ø± Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: {last_assistant_msg}
- Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ØµØ­ÙŠØ­Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„: [{valid_numbers}]

Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (JSON ÙÙ‚Ø·):
{{
  "intent": "follow_up" | "new_issue" | "resolved" | "open_new" | "other",
  "mentioned_numbers": [<strings>],
  "step_number": <int or null>,
  "new_issue_number": <string or null>,
  "confidence": <float 0..1>,
  "reason": "<Ù…Ø®ØªØµØ± ÙŠØ¨Ø±Ø± Ø§Ù„Ù‚Ø±Ø§Ø±>"
}}

Ù‚ÙˆØ§Ø¹Ø¯ Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±:
1) Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù†Ø­Ù„Ù‘Øª (Ù…Ø«Ù„: "ØªÙ… Ø­Ù„Ù‡Ø§" / "Ø§Ø´ØªØºÙ„Øª") â†’ intent="resolved".
2) Ø¥Ø°Ø§ Ø°ÙƒØ±Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø±Ù‚Ù… Ø±Ø³Ø§Ù„Ø© Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙˆÙ…Ø®ØªÙ„Ù Ø¹Ù† {current_number} â†’ intent="new_issue" ÙˆØ§Ù…Ù„Ø£ new_issue_number.
3) Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙØªØ­ Ù…Ø´ÙƒÙ„Ø©/ØªØ°ÙƒØ±Ø©/Ù‚Ø¶ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© (Ù…Ø«Ù„: "Ø§ÙØªØ­ Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø©" / "Ø§ÙØªØ­ ØªØ°ÙƒØ±Ø©" / "open new issue")
   Ø¨Ø¯ÙˆÙ† ØªØ­Ø¯ÙŠØ¯ Ø±Ù‚Ù… Ù…Ø¹Ø±ÙˆÙ â†’ intent="open_new".
4) Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ù„Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© (Ø³Ø¤Ø§Ù„/ØªÙˆØ¶ÙŠØ­/Ø°ÙƒØ± Ø®Ø·ÙˆØ©) â†’ intent="follow_up" ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ step_number Ø¥Ù† ÙˆÙØ¬Ø¯.
5) Ø¥Ù† Ù„Ù… ÙŠÙ†Ø·Ø¨Ù‚ Ø´ÙŠØ¡ Ù…Ù…Ø§ Ø³Ø¨Ù‚ â†’ "other".

Ø£Ù…Ø«Ù„Ø© Ø³Ø±ÙŠØ¹Ø©:
- "Ø§ÙØªØ­ Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø©" â†’ open_new.
- "open new ticket" â†’ open_new.
- "669" Ø£Ùˆ "Ù¦Ù¦Ù©" â†’ new_issue with new_issue_number="669".
- "Ø±Ù‚Ù… Ø®Ù…Ø³Ø©" (Ø¨Ø¹Ø¯ Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø®Ø·ÙˆØ©) â†’ follow_up, step_number=5.
- "ØªÙ… Ø­Ù„Ù‡Ø§" â†’ resolved.

Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
\"\"\"{user_text}\"\"\"\n
Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù€ JSON ÙÙ‚Ø·.
"""
    return LLMChain(
        llm=llm,
        prompt=PromptTemplate(
            template=router_prompt,
            input_variables=[
                "current_number","current_text","current_reason","current_solution",
                "last_assistant_msg","valid_numbers","user_text"
            ]
        ),
        verbose=False
    )

def _is_negative_reply(t: str) -> bool:
    s = (t or "").strip().lower()
    # Arabic & EN negatives / polite declines
    negatives = [
        "Ù„Ø§", "Ù„Ø§ Ø´ÙƒØ±Ø§", "Ù„Ø§ Ø´ÙƒØ±Ø§Ù‹", "Ø®Ù„Ø§Øµ", "Ø§Ù†ØªÙ‡Ù‰", "ØªÙ…Ø§Ù… Ø´ÙƒØ±Ø§", "Ø´ÙƒØ±Ø§Ù‹", "Ù…Ø§ Ø§Ø±ÙŠØ¯", "Ù…Ø§ Ø§Ø­ØªØ§Ø¬",
        "Ù…Ùˆ Ù„Ø§Ø²Ù…", "Ø¨Ù„Ø§", "no", "nope", "nah", "no thanks", "no thank you", "all good", "im good", "i'm good"
    ]
    return any(s == n or s.startswith(n + " ") for n in negatives)

def _asks_for_human(t: str) -> bool:
    s = (t or "").strip().lower()
    # phrases that clearly mean "connect me to human support"
    triggers = [
        "Ù…Ø³ØªØ´Ø§Ø±", "Ø§Ø¯Ø¹Ù…", "Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ", "Ø§Ù„ÙÙ†ÙŠ", "Ø§ØªÙˆØ§ØµÙ„", "ØªÙˆØ§ØµÙ„", "Ø§Ø±ÙŠØ¯ Ø§Ù„ØªÙˆØ§ØµÙ„", "Ø§Ø±ÙŠØ¯ Ø§ØªÙˆØ§ØµÙ„",
        "Ø­ÙˆÙ‘Ù„Ù†ÙŠ", "Ø­ÙˆÙ„Ù†ÙŠ", "ÙƒÙ„Ù…", "ÙƒÙ„Ù‘Ù…", "support", "human", "agent", "advisor", "escalate"
    ]
    return any(k in s for k in triggers)

def _last_msgs(n: int = 2):
    users, assistants = [], []
    for m in reversed(st.session_state.chat_history):
        if m["role"] == "assistant":
            assistants.append(m.get("content", ""))
        elif m["role"] == "user":
            users.append(m.get("content", ""))
        if len(users) >= n and len(assistants) >= n:
            break
    while len(users) < n: users.append("")
    while len(assistants) < n: assistants.append("")
    return {
        "u1": users[0], "u2": users[1] if len(users) > 1 else "",
        "a1": assistants[0], "a2": assistants[1] if len(assistants) > 1 else "",
    }


def _is_permission_related(issue: dict, user_text: str) -> bool:
    """Return True if the active issue or the user's last message looks like a permissions problem."""
    parts = [
        (issue or {}).get("message_text") or "",
        (issue or {}).get("reason") or "",
        (issue or {}).get("solution") or "",
        user_text or "",
    ]
    hay = " ".join(parts).lower()

    # Arabic + English cues
    perm_kws = [
        "ØµÙ„Ø§Ø­ÙŠØ§Øª", "ØµÙ„Ø§Ø­ÙŠØ©", "ØªÙÙˆÙŠØ¶", "ØºÙŠØ± Ù…Ø®ÙˆÙ„", "ØºÙŠØ± Ù…ØµØ±Ø­", "Ø§Ù„Ø¯ÙˆØ±", "Ø¯ÙˆØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…",
        "permission", "permissions", "authorization", "authorisation", "access denied", "role"
    ]
    return any(k in hay for k in perm_kws)


# ---------- LLM selectors (no embeddings) ----------
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

def _json_only(s: str) -> dict:
    m = re.search(r'\{.*\}', s, re.DOTALL)
    return json.loads(m.group(0)) if m else {}

@st.cache_resource
def create_llm_title_selector_chain():
    """
    Round 1: given ALL section titles (id, title, category),
    pick up to k ids that best match the user's intent.
    """
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""
Ø£Ù†Øª Ù…ÙÙ†ØªÙ‚Ù Ø°ÙƒÙŠ Ù„Ø£Ù‚Ø³Ø§Ù… Ø¯Ù„ÙŠÙ„ Onyx ERP. Ù…Ù‡Ù…ØªÙƒ Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙÙ‚Ø·.

Ù…Ø¨Ø§Ø¯Ø¦ Ø¹Ø§Ù…Ø©:
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØªØ¶Ù…Ù† "ÙƒÙŠÙ/Ø¥Ø¶Ø§ÙØ©/Ø¥Ù†Ø´Ø§Ø¡/ØªØ¹Ø±ÙŠÙ" ÙÙØ¶Ù‘Ù„ Ø§Ù„Ø´Ø§Ø´Ø§Øª ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª (ÙˆÙ„ÙŠØ³ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±).
- Ø¥Ø°Ø§ Ø°ÙÙƒØ±Øª "Ù‚Ø¨Ø¶" ÙÙØ¶Ù‘Ù„ "Ø³Ù†Ø¯ Ø§Ù„Ù‚Ø¨Ø¶" ÙˆØªØ¬Ù†Ù‘Ø¨ "Ø³Ù†Ø¯ Ø§Ù„ØµØ±Ù" Ùˆ"ØªÙ‚Ø§Ø±ÙŠØ± Ø³Ù†Ø¯ Ø§Ù„Ù‚Ø¨Ø¶".
- Ø¥Ø°Ø§ Ø°ÙÙƒØ±Øª "ØµØ±Ù" ÙÙØ¶Ù‘Ù„ "Ø³Ù†Ø¯ Ø§Ù„ØµØ±Ù" ÙˆØªØ¬Ù†Ù‘Ø¨ "ØµØ±Ù Ø¹Ù…Ù„Ø©" Ùˆ"ØªÙ‚Ø§Ø±ÙŠØ± Ø³Ù†Ø¯ Ø§Ù„ØµØ±Ù" Ù…Ø§ Ù„Ù… ÙŠÙØ°ÙƒØ± "Ø¹Ù…Ù„Ø©".
- Ø¥Ø°Ø§ Ø°ÙÙƒØ±Øª "Ø¹Ù…Ù„Ø©" ÙÙØ¶Ù‘Ù„ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª "ØµØ±Ù Ø¹Ù…Ù„Ø©/Ø·Ù„Ø¨ ØµØ±Ù Ø¹Ù…Ù„Ø©".
- "Ù…ÙˆØ±Ø¯" ÙŠÙ„Ù…Ø­ ØºØ§Ù„Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø³Ù†Ø¯ ØµØ±ÙØŒ "Ø¹Ù…ÙŠÙ„" ÙŠÙ„Ù…Ø­ ØºØ§Ù„Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø³Ù†Ø¯ Ù‚Ø¨Ø¶.
- Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… "ØªÙ‚Ø±ÙŠØ±" ÙØ§Ø®ØªØ± ØªÙ‚Ø§Ø±ÙŠØ±ØŒ ÙˆØ¥Ù„Ø§ ÙÙØ¶Ù‘Ù„ Ø§Ù„Ø´Ø§Ø´Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ©.

Ø³Ø¤Ø§Ù„/Ù‡Ø¯Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
{query}

Ø§Ù„Ø¹Ù†Ø§ØµØ± (id | title | category):
{items}

Ø£Ø±Ø¬Ø¹ JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:
{{
  "ids": [<up to {k} integers>],
  "short_reason": "<Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±Ø§Øª>"
}}
""",
        input_variables=["query", "items", "k"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

@st.cache_resource
def create_llm_candidate_ranker_chain():
    """
    Round 2: given small candidate set with snippets, rank to top 3.
    """
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""
Ø£Ù†Øª Ù…ÙØ¹ÙŠØ¯ ØªØ±ØªÙŠØ¨ Ø¯Ù‚ÙŠÙ‚. Ù„Ø¯ÙŠÙƒ Ù…Ø±Ø´Ù‘Ø­Ø§Øª ØµØºÙŠØ±Ø© Ù…Ù† Ø¯Ù„ÙŠÙ„ Onyx ERP.
Ø±ØªÙ‘Ø¨Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ØŒ ÙˆØ§Ø­Ø°Ø± Ù…Ù† Ø§Ù„Ø®Ù„Ø· Ø¨ÙŠÙ† "Ø³Ù†Ø¯" Ùˆ"Ø·Ù„Ø¨" Ùˆ"ØªÙ‚Ø±ÙŠØ±".

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
{query}

Ø§Ù„Ù…Ø±Ø´Ø­Ø§Øª (id | title | category | snippet):
{cands}

Ø£Ø¬Ø¨ JSON ÙÙ‚Ø·:
{{
  "ranked_ids": [<Ø£ÙØ¶Ù„ 3 ids Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨>],
  "confidence": 0.0_to_1.0,
  "short_reason": "<Ù„Ù…Ø§Ø°Ø§>"
}}
""",
        input_variables=["query", "cands"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

def is_follow_up_on_current_issue(user_text: str) -> bool:
    issue = st.session_state.get("current_issue")
    if not (st.session_state.get("solution_provided") and issue):
        return False
    text = (user_text or "").strip()
    msg_no = str(issue.get("message_number", "")).strip()
    if msg_no and re.search(rf'\b{re.escape(msg_no)}\b', text):
        return True
    followup_cues = [
        "Ù„Ù… Ø£Ø³ØªØ·Ø¹","Ù…Ø´ ÙˆØ§Ø¶Ø­","ØºÙŠØ± ÙˆØ§Ø¶Ø­","ÙƒÙŠÙ","Ø£Ø´Ø±Ø­","Ù…Ø§ Ø§Ù„Ù…Ù‚ØµÙˆØ¯","Ù„Ù… ÙŠØ¹Ù…Ù„",
        "Ù…Ø§ Ø²Ø§Ù„","Ù„Ø§ ÙŠØ²Ø§Ù„","Ø£Ø¹Ø¯","Ø±Ø¬Ø§Ø¡","Ø§Ù„Ù…Ø²ÙŠØ¯","Ù„Ù… ÙŠØªÙ…","Ø£ÙŠÙ† Ø£Ø¬Ø¯","Ø®Ø·ÙˆØ©",
        "Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ","Ø«Ù…","Ù…Ø§Ø°Ø§ Ø£ÙØ¹Ù„","Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©","Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø·ÙˆØ©","Ù„Ù… Ø£ØªÙ…ÙƒÙ†"
    ]
    if any(p in text for p in followup_cues):
        nums = re.findall(r'\d+', text)
        if not nums or (msg_no and all(n == msg_no for n in nums)):
            return True
    try:
        embedding_model = get_embedding_model()
        query_emb = embedding_model.encode(text, convert_to_tensor=True)
        ref_text = " ".join([
            issue.get("message_text","") or "",
            issue.get("reason","") or "",
            issue.get("solution","") or ""
        ]).strip()
        ref_emb = torch.tensor(issue.get("embedding")) if issue.get("embedding") else embedding_model.encode(ref_text, convert_to_tensor=True)
        sim = util.cos_sim(query_emb, ref_emb).item()
        return sim >= 0.35
    except Exception:
        return False

def llm_parse_page_as_json(page_image_bytes):
    llm = get_llm_model()
    prompt = """
    Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ø­Ù„Ù„ ØµÙˆØ±Ø© Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ø¯Ù„ÙŠÙ„ ØªÙ‚Ù†ÙŠ.
    Ù„ÙƒÙ„ Ù‚Ø³Ù… Ø®Ø·Ø£ ØªØ¬Ø¯Ù‡ØŒ Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
    - message_number
    - message_text
    - location
    - reason
    - solution
    - note (Ø¥Ù† ÙˆØ¬Ø¯Øª)
    Ø£Ø±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© JSON ÙÙ‚Ø·. Ø¥Ù† Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø´ÙŠØ¡ ÙØ£Ø±Ø¬Ø¹ [].
    """
    encoded_image = base64.b64encode(page_image_bytes).decode('utf-8')
    message = HumanMessage(content=[{"type": "text", "text": prompt}, {"type": "image_url", "image_url": {
        "url": f"data:image/png;base64,{encoded_image}"}}])
    try:
        response = llm.invoke([message])
        json_text = re.search(r'\[.*\]', response.content, re.DOTALL).group(0)
        return json.loads(json_text)
    except Exception as e:
        st.warning(f"AI parsing failed for a page, skipping. Error: {e}")
        return []

def llm_extract_message_number_from_image(image_bytes):
    llm = get_llm_model()
    prompt = """
    Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù….
    Ø§Ù†Ø¸Ø± Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ Ù‡Ø°Ù‡. Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø¬ÙˆØ§Ø± "Message No -".
    Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø§Ù„Ø±Ù‚Ù…ØŒ Ø£Ø±Ø¬Ø¹Ù‡ ÙÙ‚Ø· ÙƒØ¹Ø¯Ø¯ ØµØ­ÙŠØ­. Ø¥Ø°Ø§ Ù„Ù… ØªØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡ØŒ Ø£Ø±Ø¬Ø¹ "null".
    Ù„Ø§ ØªÙ‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠØŒ ÙÙ‚Ø· Ø§Ù„Ø±Ù‚Ù… Ø£Ùˆ "null".
    """
    encoded_image = base64.b64encode(image_bytes).decode('utf-8')
    message = HumanMessage(content=[{"type": "text", "text": prompt}, {"type": "image_url", "image_url": {
        "url": f"data:image/png;base64,{encoded_image}"}}])
    try:
        response = llm.invoke([message])
        cleaned_response = response.content.strip()
        if cleaned_response.isdigit():
            return cleaned_response
        return None
    except Exception as e:
        st.error(f"Error extracting message number from image: {e}")
        return None

def generate_instructional_response(result):
    llm = get_llm_model()
    context = f"""
    - Error Message Text: {result.get('message_text')}
    - Reason: {result.get('reason')}
    - Solution: {result.get('solution')}
    - Note: {result.get('note', 'N/A')}
    """
    prompt_template = """
    Ø£Ù†Øª ÙˆÙƒÙŠÙ„ Ø¯Ø¹Ù… ÙÙ†ÙŠ Ø®Ø¨ÙŠØ± ÙˆÙ…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙÙŠ Ù†Ø¸Ø§Ù… "Onyx ERP". Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø¯Ù„ÙŠÙ„ Ø¥Ø±Ø´Ø§Ø¯ÙŠ ÙˆØ§Ø¶Ø­ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù….
    Ø§ØªØ¨Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø¯Ù‚Ø©:
    1. Ø®Ø§Ø·Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø£Ø³Ù„ÙˆØ¨ ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ù‡Ù†ÙŠ.
    2. Ø§Ø³ØªØ®Ø¯Ù… ØªÙ†Ø³ÙŠÙ‚ Markdown Ø§Ù„ØªØ§Ù„ÙŠ: `### ğŸ” Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ­Ø¯Ø«ØŸ` Ùˆ `### ğŸ› ï¸ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø¥ØµÙ„Ø§Ø­`.
    3. ØªØ­Øª "Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ­Ø¯Ø«ØŸ"ØŒ Ø§Ø´Ø±Ø­ Ø³Ø¨Ø¨ Ø§Ù„Ø®Ø·Ø£ Ø¨Ø¹Ø¨Ø§Ø±Ø§Øª Ø¨Ø³ÙŠØ·Ø©.
    4. ØªØ­Øª "ÙƒÙŠÙÙŠØ© Ø§Ù„Ø¥ØµÙ„Ø§Ø­"ØŒ Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø­Ù„ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø±Ù‚Ù…Ø© ÙˆÙˆØ§Ø¶Ø­Ø©.
    5. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ "Ù…Ù„Ø§Ø­Ø¸Ø©"ØŒ Ø£Ø¶ÙÙ‡Ø§ ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© ØªØ­Øª Ø¹Ù†ÙˆØ§Ù† **"ğŸ’¡ Ù…Ù„Ø§Ø­Ø¸Ø© Ù‡Ø§Ù…Ø©:"**.
    **Ø§Ù„Ø³ÙŠØ§Ù‚:**
    {context}
    Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
    """
    final_prompt = prompt_template.format(context=context)
    response = llm.invoke(final_prompt)
    image_path = result.get('image_path')
    return {"answer": response.content, "images": [image_path] if image_path else []}

def classify_intent(user_text: str) -> dict:
    text = (user_text or "").strip().lower()
    resolved_cues = ["ØªÙ… Ø­Ù„Ù‡Ø§", "Ø§Ù†Ø­Ù„Øª", "Ø§Ø´ØªØºÙ„Øª", "thank you", "solved", "works now"]
    if any(cue in text for cue in resolved_cues):
        return {"intent": "resolved", "confidence": 0.95}
    issue = st.session_state.get("current_issue") or {}
    curr_no = str(issue.get("message_number", "")).strip()
    nums = re.findall(r'\d+', text)
    if nums and (not curr_no or any(n != curr_no for n in nums)):
        return {"intent": "new_issue", "confidence": 0.85}
    try:
        llm = get_llm_model()
        context = {
            "current_message_number": curr_no,
            "current_message_text": (issue.get("message_text") or "")[:1000],
            "current_reason": (issue.get("reason") or "")[:800],
            "current_solution": (issue.get("solution") or "")[:1200],
        }
        prompt = f"""
Ø£Ù†Øª Ù…ØµÙ†Ù Ù†ÙˆØ§ÙŠØ§ Ø±Ø³Ø§Ø¦Ù„ Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ø¯Ø¹Ù… ÙÙ†ÙŠ Ù„Ù†Ø¸Ø§Ù… Onyx ERP.
Ù„Ø¯ÙŠÙ†Ø§ Ù…Ø´ÙƒÙ„Ø© Ù†Ø´Ø·Ø© Ø¨Ø±Ù‚Ù…: {context["current_message_number"] or "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"}.

ØµÙ†Ù‘Ù Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¥Ù„Ù‰ ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ù…Ù† Ø§Ù„Ù‚ÙŠÙ…:
- follow_up
- new_issue
- resolved
- other

Ø§Ù„Ù…Ø¹Ø·ÙŠØ§Øª Ø¹Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© (Ø§Ø®ØªØµØ§Ø±):
[Ø§Ù„Ù†Øµ]: {context["current_message_text"]}
[Ø§Ù„Ø³Ø¨Ø¨]: {context["current_reason"]}
[Ø§Ù„Ø­Ù„]: {context["current_solution"]}

Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: \"\"\"{user_text}\"\"\"

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø·:
{{"intent": "<follow_up|new_issue|resolved|other>", "confidence": 0.0}}
        """.strip()
        resp = llm.invoke(prompt)
        m = re.search(r'\{.*\}', resp.content, re.DOTALL)
        if m:
            data = json.loads(m.group(0))
            if isinstance(data, dict) and "intent" in data:
                return {"intent": data.get("intent"), "confidence": float(data.get("confidence", 0.6))}
    except Exception:
        pass
    if len(text) <= 60 or any(k in text for k in ["ÙƒÙŠÙ","Ù…Ø§","Ù…Ø´ ÙˆØ§Ø¶Ø­","Ù„Ù… Ø£Ø³ØªØ·Ø¹","Ù„Ù… ÙŠØ¹Ù…Ù„","Ø®Ø·ÙˆØ©"]):
        return {"intent": "follow_up", "confidence": 0.6}
    return {"intent": "other", "confidence": 0.5}



@st.cache_resource
def create_guide_reranker_chain():
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""
Ø£Ù†Øª Ù…ØµÙ†Ù ÙŠØ®ØªØ§Ø± Ø£ÙØ¶Ù„ Ù‚Ø³Ù… Ù…Ù† Ø¯Ù„ÙŠÙ„ Ù…Ø³ØªØ®Ø¯Ù… Onyx ERP (GL) Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù†ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….

Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª:
- Ø³Ø¤Ø§Ù„/Ù‡Ø¯Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {query}
- Ø§Ù„Ù…Ø±Ø´Ù‘Ø­Ø§Øª (Ø£Ù‚ØµÙ‰ 6). Ù„ÙƒÙ„ Ù…Ø±Ø´Ù‘Ø­: id, title, snippet (Ù…Ù† Ø£ÙˆÙ„ Ù†Øµ Ø§Ù„ØµÙØ­Ø©)

Ø§Ø®ØªØ± Ø£ÙØ¶Ù„ Ù…Ø±Ø´Ù‘Ø­ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·ØŒ Ø£Ùˆ "none" Ø¥Ù† Ù„Ù… ÙŠÙƒÙ† Ø£ÙŠ Ù…Ø±Ø´Ù‘Ø­ Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹ Ø¨ÙˆØ¶ÙˆØ­.

Ø£Ø¬Ø¨ JSON ÙÙ‚Ø·:
{{
  "best_id": "<id Ø£Ùˆ none>",
  "confidence": 0.0,
  "short_reason": "<Ø³Ø¨Ø¨ Ù…Ø®ØªØµØ±>"
}}

Ø§Ù„Ù…Ø±Ø´Ù‘Ø­Ø§Øª:
{candidates}
""",
        input_variables=["query", "candidates"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

@st.cache_resource
def create_troubleshooting_chain():
    llm = get_llm_model()
    prompt_template = """
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¯Ø¹Ù… Ù„Ù†Ø¸Ø§Ù… "Onyx ERP" ÙˆØªØ¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù†Ø´Ø·Ø© ÙÙ‚Ø· (Ù„Ø§ ØªÙØªØ­ Ù‚Ø¶Ø§ÙŠØ§ Ø¬Ø¯ÙŠØ¯Ø©).
Ù‚Ø¯Ù‘Ù… Ø¥Ø±Ø´Ø§Ø¯Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ø¨Ø³Ø¤Ø§Ù„ ØªØ­Ù‚Ù‚ ÙˆØ§Ø­Ø¯ ÙÙŠ ÙƒÙ„ Ø±Ø³Ø§Ù„Ø©.

Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:
- Ø±Ù‚Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø©: {message_number}
- Ø§Ù„Ù†Øµ: {message_text}
- Ø§Ù„Ø³Ø¨Ø¨: {reason}
- Ø§Ù„Ø­Ù„ Ø§Ù„Ù…Ù‚ØªØ±Ø­: {solution}

Ù…Ø¨Ø§Ø¯Ø¦ Ø°ÙƒØ§Ø¡ Ø¹Ø§Ù…Ø© Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„ ÙÙŠ Ø®Ø·ÙˆØ©:
- ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª/Ø§Ù„Ø¯ÙˆØ± ÙˆØ§Ù„ÙØ±Ø¹ ÙˆØ§Ù„Ø´Ø±ÙƒØ©.
- ØªØ¹Ø§Ø±Ø¶ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª: Ø®ÙŠØ§Ø± ÙŠÙØ¹Ø·Ù„ Ø®ÙŠØ§Ø±Ù‹Ø§ Ø¢Ø®Ø±.
- Ø¶Ø±ÙˆØ±Ø© Ø§Ù„Ø­ÙØ¸ ÙˆØ¥Ø¹Ø§Ø¯Ø© ÙØªØ­ Ø§Ù„Ø´Ø§Ø´Ø©/ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¬Ù„Ø³Ø©/Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ø´.
- Ø¨Ø·Ø¡/Ø§Ù†Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø§ØªØµØ§Ù„ Ø£Ùˆ Ù‚ÙÙ„ Ø§Ù„Ø³Ø¬Ù„.
- Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©/Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ ØªÙ…Ù†Ø¹ Ø§Ù„ØªÙØ¹ÙŠÙ„.

Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}

Ø¥Ù† Ø´Ø¹Ø±Øª Ø£Ù† Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ø³ØªÙÙ†ÙØ¯ØªØŒ Ø§ÙƒØªØ¨ ÙÙ‚Ø·: "Ù†Ø­ØªØ§Ø¬ ØªØµØ¹ÙŠØ¯".
Ø±Ø¯ Ù…ÙˆØ¬Ø² Ù…Ø¨Ø§Ø´Ø±:
"""
    return LLMChain(
        llm=llm,
        prompt=PromptTemplate(
            template=prompt_template,
            input_variables=["message_number","message_text","reason","solution","chat_history","question"]
        ),
        verbose=False
    )





def build_gl_guide_kb(pdf_path: str) -> bool:
    """
    Final version: Correctly handles image extraction for topics that start
    on the same page where a previous topic ends, preventing image mismatch.
    """
    try:
        from sentence_transformers import SentenceTransformer
        _HAS_EMB = True
        print("SentenceTransformer library found. Embeddings will be generated.")
    except ImportError:
        _HAS_EMB = False
        print("Warning: SentenceTransformer not found. Run 'pip install sentence-transformers' to generate embeddings.")

    # --- Configuration ---
    base_dir = Path(__file__).parent
    cache_dir = base_dir / "cache"
    guide_kb_path = cache_dir / "gl_guide_kb.json"
    guide_img_dir = cache_dir / "guide_images"
    os.makedirs(guide_img_dir, exist_ok=True)

    # --- Helper Functions ---
    def clean_body_text(s: str) -> str:
        s = s or ""
        s = re.sub(r'solution_image_start:.*?solution_image_end:', '', s, flags=re.DOTALL | re.IGNORECASE)
        s = re.sub(r'\s+', ' ', s.replace("\r", "")).strip()
        return s

    def save_image(doc, xref, out_dir: Path, topic_title: str, img_idx: int):
        # Clean the title to use in the filename
        temp_title = re.sub(r'[^a-zA-Z0-9\s]', '', topic_title)
        safe_title = re.sub(r'\s+', '_', temp_title).lower().strip('_')[:30]
        if not safe_title:
            safe_title = "guide_topic"  
        
        try:
            img = doc.extract_image(xref)
            ext = img.get("ext", "png").lower()
            # Use a more descriptive filename
            # Inside the save_image function in build_gl_guide_kb
            out_path = out_dir / f"{safe_title}_img_{img_idx}.{ext}"
            with open(out_path, "wb") as f:
                f.write(img["image"])
            return str(out_path)
        except Exception:
            return None

    def get_marker_pos(doc, page_range, marker_text, after_pos=None):
        # Finds the first occurrence of a marker AFTER a specific starting position.
        for page_num in page_range:
            # Skip pages before the starting position's page
            if after_pos and page_num < after_pos["page"]:
                continue

            page = doc[page_num]
            rects = page.search_for(marker_text, quads=False)
            for rect in rects:
                # If on the same page, ensure the marker is vertically below the start
                if after_pos and page_num == after_pos["page"] and rect.y0 < after_pos["y"]:
                    continue
                # This is the first valid marker found
                return {"page": page_num, "y": float(rect.y0)}
        return None

    def collect_images_for_topic(doc, page_range, topic_start_pos, topic_title):
        images_found = []
        # **MODIFICATION**: The search for the start marker now begins AFTER the topic_title
        start_marker = get_marker_pos(doc, page_range, "solution_image_start:", after_pos=topic_start_pos)
        if not start_marker:
            return []

        end_marker = get_marker_pos(doc, page_range, "solution_image_end:", after_pos=start_marker)
        if not end_marker:
            end_marker = {"page": page_range.stop - 1, "y": 9999}

        start_page, end_page = start_marker["page"], end_marker["page"]
        save_idx = 0

        for p_num in range(start_page, end_page + 1):
            page = doc[p_num]
            for img_info in page.get_images(full=True):
                xref = img_info[0]
                img_bbox = page.get_image_bbox(img_info)

                is_after_start = (p_num > start_page) or (p_num == start_page and img_bbox.y1 > start_marker["y"])
                is_before_end = (p_num < end_page) or (p_num == end_page and img_bbox.y0 < end_marker["y"])

                if is_after_start and is_before_end:
                    saved_path = save_image(doc, xref, guide_img_dir, topic_title, save_idx)
                    if saved_path:
                        images_found.append(saved_path)
                        save_idx += 1
        return images_found

    # --- Main Execution ---
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"Error: Could not open PDF file '{pdf_path}': {e}")
        return False

    topic_re = re.compile(
        r"(?P<ar_title>.*?)\s*topic_title:\s*(?P<eng_title>.*?)\n"
        r"category:\s*(?P<category>.*?)\n"
        r"answer_start:\s*(?P<answer_content>.*?)\s*answer_end:",
        re.DOTALL | re.IGNORECASE
    )

    full_text = "".join(page.get_text().replace("\r", "") + "\n" for page in doc)

    char_count = 0
    page_map = []
    for i, page in enumerate(doc):
        text_len = len(page.get_text().replace("\r", "")) + 1
        page_map.append({'start': char_count, 'end': char_count + text_len, 'page_num': i})
        char_count += text_len

    def get_pos_for_char(char_index):
        page_num = next((p['page_num'] for p in page_map if p['start'] <= char_index < p['end']), 0)
        page_start_char = page_map[page_num]['start']
        # This is an approximation for the Y coordinate, but it's sufficient for ordering
        y_coord_approx = (char_index - page_start_char)
        return {"page": page_num, "y": y_coord_approx}

    def get_pages_for_match(match):
        start_char, end_char = match.span()
        start_page = get_pos_for_char(start_char)["page"]
        end_page = get_pos_for_char(end_char)["page"]
        return range(start_page, end_page + 1)

    matches = list(topic_re.finditer(full_text))
    if not matches:
        print("Extraction failed: Could not find any topics matching the specified structure.")
        return False

    print(f"Found {len(matches)} topics. Extracting content...")

    sections = []
    for m in matches:
        ar_title = m.group("ar_title").strip()
        eng_title = m.group("eng_title").strip().replace('(', '( ').replace(')', ' )')  # Clean up parens
        eng_title = re.sub(r'\s+', ' ', eng_title)

        full_title = f"{ar_title} / {eng_title}" if ar_title and eng_title else ar_title or eng_title


        print(f" - Processing: {full_title}")

        page_range = get_pages_for_match(m)

        # Get the precise starting position of this topic's title
        topic_start_pos = get_marker_pos(doc, page_range, "topic_title:")

        # Pass this starting position to the image collector
        images = collect_images_for_topic(doc, page_range, topic_start_pos, full_title)

        raw_answer_content = m.group("answer_content")

        # keep old: ar_title, eng_title already parsed by your regex
        full_title = f"{ar_title} / {eng_title}" if ar_title and eng_title else ar_title or eng_title

        title_norm_ar = ar_norm(ar_title)
        title_norm_en = (eng_title or "").strip().lower()

        def infer_doc_type():
            # Any obvious "ØªÙ‚Ø§Ø±ÙŠØ±" -> report, else screen (procedure)
            if "ØªÙ‚Ø§Ø±ÙŠØ±" in ar_title or "reports" in title_norm_en:
                return "report"
            return "screen"

        def infer_voucher_type():
            if has_any(ar_title, SYN["receipt"]): return "receipt"
            if has_any(ar_title, SYN["payment"]): return "payment"
            return None

        def infer_object_key():
            # map from the title (ar/en) without hardcoding every page
            if has_any(ar_title, SYN["bank"]) or "bank" in title_norm_en:
                return "bank"
            if has_any(ar_title, SYN["cash_fund"]) or "cash fund" in title_norm_en or "cash funds" in title_norm_en:
                return "cash_fund"
            if has_any(ar_title, SYN["checkbook"]) or "checkbook" in title_norm_en:
                return "checkbook"
            if "receipt voucher" in title_norm_en:
                return "receipt_voucher"
            if "payment voucher" in title_norm_en:
                return "payment_voucher"
            # fallback None; not every screen is a simple object
            return None

        sections.append({
            "title": full_title,
            "ar_title": ar_title,
            "en_title": eng_title,
            "category": m.group("category").strip(),
            "body": clean_body_text(raw_answer_content),
            "images": images,
            # NEW lightweight ontology fields
            "doc_type": infer_doc_type(),
            "voucher_type": infer_voucher_type(),
            "object_key": infer_object_key(),
        })

    # --- Generate Embeddings ---
    if _HAS_EMB and sections:
        try:
            print("\nGenerating embeddings...")
            model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            texts_to_embed = [f"Title: {s['title']}\nCategory: {s['category']}\nContent: {s['body']}" for s in sections]
            embeddings = model.encode(texts_to_embed, show_progress_bar=True)
            for i, section in enumerate(sections):
                section["embedding"] = embeddings[i].tolist()
            print("Embeddings generated successfully.")
        except Exception as e:
            print(f"Error during embedding generation: {e}")

    with open(guide_kb_path, "w", encoding="utf-8") as f:
        json.dump(sections, f, ensure_ascii=False, indent=2)

    print(f"\nSuccessfully built the GL Guide KB with {len(sections)} topics.")
    return True

def load_gl_guide_kb() -> bool:
    if GUIDE_KB_PATH.exists():
        try:
            with open(GUIDE_KB_PATH, "r", encoding="utf-8") as f:
                st.session_state.gl_guide_kb = json.load(f)
            return True
        except Exception:
            return False
    return False



def build_errors_kb(pdf_path: str) -> bool:
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        st.error(f"ØªØ¹Ø°Ø± ÙØªØ­ Ù…Ù„Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ø­Ù„ÙˆÙ„: {e}")
        return False

    all_items = []
    for i, page in enumerate(doc):
        # Extract last image on the page as the "Solution Image" (if any)
        solution_image_path = None
        images_on_page = page.get_images(full=True)
        if images_on_page:
            try:
                base_image = doc.extract_image(images_on_page[-1][0])
                # reuse IMAGE_CACHE_DIR just like your old pipeline
                out_path = IMAGE_CACHE_DIR / f"errors_p{i+1}_solution.png"
                with open(out_path, "wb") as f:
                    f.write(base_image["image"])
                solution_image_path = str(out_path)
            except Exception:
                pass

        # Parse structured error data from page
        try:
            pix = page.get_pixmap(dpi=150)
            page_image_bytes = pix.tobytes("png")
            parsed = llm_parse_page_as_json(page_image_bytes)
        except Exception:
            parsed = []

        # Attach image_path to each parsed item
        for it in parsed:
            if solution_image_path:
                it["image_path"] = solution_image_path
            all_items.append(it)

    if not all_items:
        st.warning("Ù„Ù… Ø£Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ø±Ø³Ø§Ø¦Ù„ Ø®Ø·Ø£ Ù…Ù†Ø¸Ù…Ø© ÙÙŠ Ù…Ù„Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ø­Ù„ÙˆÙ„.")
        return False

    embedding_model = get_embedding_model()
    texts = [f"{p.get('message_text','')} {p.get('reason','')}" for p in all_items]
    embs = embedding_model.encode(texts, convert_to_tensor=True)
    for i, p in enumerate(all_items):
        p["embedding"] = embs[i].tolist()

    with open(ERRORS_KB_PATH, "w", encoding="utf-8") as f:
        json.dump(all_items, f, ensure_ascii=False, indent=2)

    # refresh valid numbers for router/safety
    st.session_state.valid_message_numbers = {
        str(p.get('message_number','')).strip()
        for p in all_items if str(p.get('message_number','')).strip()
    }
    return True

def load_errors_kb() -> bool:
    if ERRORS_KB_PATH.exists():
        try:
            with open(ERRORS_KB_PATH, "r", encoding="utf-8") as f:
                st.session_state.gl_errors_kb = json.load(f)
            st.session_state.valid_message_numbers = {
                str(p.get('message_number','')).strip()
                for p in st.session_state.gl_errors_kb if str(p.get('message_number','')).strip()
            }
            return True
        except Exception:
            return False
    return False


def load_knowledge_base():
    if KB_PATH.exists():
        try:
            with open(KB_PATH, "r", encoding="utf-8") as f:
                st.session_state.knowledge_base = json.load(f)

            st.session_state.valid_message_numbers = {
                str(p.get('message_number', '')).strip()
                for p in st.session_state.knowledge_base
                if str(p.get('message_number', '')).strip()
            }
            return True
        except:
            return False
    return False

# >>> NEW: helper to build CSV snapshot of the case
def _build_case_csv_bytes(user_query: str) -> bytes:
    issue = st.session_state.get("current_issue") or {}
    meta = {
        "message_number": str(issue.get("message_number", "")),
        "message_text": (issue.get("message_text") or "").strip(),
        "reason": (issue.get("reason") or "").strip(),
        "solution": (issue.get("solution") or "").strip(),
        "note": (issue.get("note") or "").strip(),
        "escalated_at_utc": datetime.now(timezone.utc).isoformat(),
        "follow_up_attempts": str(st.session_state.get("follow_up_attempts", 0)),
    }

    buf = StringIO()
    w = csv.writer(buf)
    # Meta header section
    w.writerow(["field", "value"])
    for k, v in meta.items():
        w.writerow([k, v])
    # User last turn (trigger)
    w.writerow([])
    w.writerow(["trigger_user_query", user_query])

    # Conversation log
    w.writerow([])
    w.writerow(["role", "content"])
    for m in st.session_state.get("chat_history", []):
        role = m.get("role", "")
        content = (m.get("content", "") or "").replace("\n", " ").strip()
        # Avoid dumping raw image bytes; mark presence instead
        if m.get("images"):
            content = f"{content} [attached_image]"
        w.writerow([role, content])

    # UTF-8 with BOM so Arabic is Excel-friendly
    return ("\ufeff" + buf.getvalue()).encode("utf-8-sig")

# >>> NEW: upload to GCS and return gs:// URI (or None)
def _upload_case_to_gcs(user_query: str):
    if not GCS_BUCKET_NAME:
        return None  # not configured; skip quietly

    try:
        # Lazy import so your app runs even if google libs aren't installed
        from google.cloud import storage
        from google.oauth2 import service_account

        # Build client
        if GCS_CREDENTIALS_JSON and os.path.exists(GCS_CREDENTIALS_JSON):
            creds = service_account.Credentials.from_service_account_file(GCS_CREDENTIALS_JSON)
            client = storage.Client(credentials=creds, project=creds.project_id)
        else:
            client = storage.Client()  # ADC

        bucket = client.bucket(GCS_BUCKET_NAME)

        issue = st.session_state.get("current_issue") or {}
        msg_no = str(issue.get("message_number", "") or "unknown")
        uid = uuid4().hex[:8]
        day = datetime.utcnow().strftime("%Y/%m/%d")
        object_name = f"escalations/{day}/case_{msg_no}_{uid}.csv"

        data = _build_case_csv_bytes(user_query)
        blob = bucket.blob(object_name)
        blob.upload_from_string(data, content_type="text/csv")

        return f"gs://{GCS_BUCKET_NAME}/{object_name}"
    except Exception as e:
        # Donâ€™t break the UX; just log a warning for admins/devs
        st.warning(f"ØªØ¹Ø°Ø± Ø±ÙØ¹ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµØ¹ÙŠØ¯ Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©: {e}")
        return None

def _escalate_and_reset(user_query: str, preface: str = None):
    # Upload snapshot before resetting state
    gcs_uri = _upload_case_to_gcs(user_query)
    if gcs_uri:
        st.session_state.last_escalation_uri = gcs_uri

    parts = []
    if preface:
        parts.append(preface.strip())

    parts.append(
        "ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ùƒ Ù…Ø­Ø¬ÙˆØ¨ Ø¹Ù† Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø±Ø³Ø§Ù„Ø©. Ø³Ø£Ø±Ø¨Ø·Ùƒ Ø§Ù„Ø¢Ù† Ø¨Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±:\n"
        "- **Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ:** support@example.com\n"
        "- **Ø§Ù„Ù‡Ø§ØªÙ:** 123-456-7890\n"
        "ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø© Ù„Ø¯ÙŠÙ†Ø§ Ù„ÙŠØ·Ù‘Ù„Ø¹ Ø¹Ù„ÙŠÙ‡Ø§ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±."
    )
    reply = "\n\n".join(parts)

    st.session_state.chat_history.append({"role": "assistant", "content": reply})
    st.session_state.memory.save_context({"question": user_query}, {"output": reply})

    # reset state
    st.session_state.solution_provided = False
    st.session_state.active_troubleshooting = False
    st.session_state.follow_up_attempts = 0
    st.session_state.awaiting_anything_else = False
    st.session_state.current_issue = None
    st.session_state.current_issue_embedding = None
    st.rerun()

def to_english_digits(s: str) -> str:
    trans = str.maketrans("Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©", "0123456789")
    return s.translate(trans)

def get_text_embedding(text: str):
    model = get_embedding_model()
    return model.encode(text, convert_to_tensor=True)




@st.cache_resource
def create_anything_else_guard_chain():
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""
Ø£Ù†Øª Ù…ØµÙ†Ù‘Ù Ù†ÙŠØ© Ø¨Ø¹Ø¯ Ø¥ØºÙ„Ø§Ù‚ Ù…Ø´ÙƒÙ„Ø© Ø³Ø§Ø¨Ù‚Ø©. Ù‚Ø±Ù‘Ø± Ù‡Ù„ ÙŠØ±ÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
- ÙØªØ­ Ù…Ø´ÙƒÙ„Ø©/ØªØ°ÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ø¢Ù†  => "open_new"
- Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ù„Ø§ØŒ Ø´ÙƒØ±Ø§Ù‹ØŒ Ø®Ù„Ø§Øµâ€¦) => "done"
- Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©             => "unclear"

Ø£Ø®Ø±Ø¬ JSON ÙÙ‚Ø·:
{{
  "decision": "open_new" | "done" | "unclear",
  "new_issue_number": <string or null>,   // Ø¥Ù† ÙˆÙØ¬Ø¯ Ø±Ù‚Ù… Ø±Ø³Ø§Ù„Ø© ÙÙŠ Ø§Ù„Ù†Øµ (Ø£Ø±Ù‚Ø§Ù… Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)
  "confidence": <float 0..1>
}}

Ø¥Ø±Ø´Ø§Ø¯Ø§Øª:
- Ø¥Ù† Ø°ÙƒØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø±Ù‚Ù… Ø±Ø³Ø§Ù„Ø© (Ù…Ø«Ù„ 939 Ø£Ùˆ Ù¡Ù¤Ù©Ù¥) ÙØ§Ø¹ØªØ¨Ø± Ø§Ù„Ù‚Ø±Ø§Ø± "open_new" ÙˆØ§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø±Ù‚Ù….
- ØµÙŠØº Ù…Ù‚Ø¨ÙˆÙ„Ø© Ù„Ù€ÙØªØ­ Ø¬Ø¯ÙŠØ¯: "Ø§ÙØªØ­ ØªØ°ÙƒØ±Ø©/Ù…Ø´ÙƒÙ„Ø©", "Ø£Ø¨Ø¯Ø£ Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø©", "Ù‚Ø¶ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©", "start a new ticket/issue", "open new", "another error", Ø¥Ù„Ø®.
- ØµÙŠØº Ø§Ù„Ø¥Ù†Ù‡Ø§Ø¡: "Ù„Ø§", "Ù„Ø§ Ø´ÙƒØ±Ø§Ù‹", "Ø®Ù„Ø§Øµ", "ØªÙ…", "ØªÙ…Ø§Ù…", "thanks", "no thanks", Ø¥Ù„Ø®.
- Ø£Ù…Ø«Ù„Ø© Ø³Ø±ÙŠØ¹Ø©:
  - "Ù„Ø§ Ø´ÙƒØ±Ø§Ù‹" -> done
  - "Ø§ÙØªØ­ ØªØ°ÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©" -> open_new
  - "939" -> open_new + new_issue_number="939"
  - "Ù†Ø¹Ù…" Ø¨Ø¹Ø¯ Ø³Ø¤Ø§Ù„ "Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø´ÙŠØ¦Ø§Ù‹ Ø¢Ø®Ø±ØŸ" -> open_new (Ø¥Ù† Ù„Ù… ØªÙØ°ÙƒØ± Ù…Ø¹Ù„ÙˆÙ…Ø© Ø£Ø®Ø±Ù‰)
  - "Ø´ÙƒØ±Ø§Ù‹" -> done

Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
\"\"\"{user_text}\"\"\"

Ø£Ø¬Ø¨ JSON ÙÙ‚Ø·.
""",
        input_variables=["user_text"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

def get_issue_reference_embedding(issue: dict):
    if st.session_state.current_issue_embedding is not None:
        return st.session_state.current_issue_embedding
    ref_text = " ".join([
        issue.get("message_text", "") or "",
        issue.get("reason", "") or "",
        issue.get("solution", "") or ""
    ]).strip()
    emb = get_text_embedding(ref_text)
    st.session_state.current_issue_embedding = emb
    return emb

@st.cache_resource
def create_llm_passage_extractor_chain():
    """
    Takes a user query and a large document text and extracts the single
    most relevant passage that answers the query.
    """
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""You are an expert data extractor. You are given a user's question and the full text of a relevant document section. Your task is to find and extract the specific paragraph, sentence, or definition that directly answers the user's question from within the document text.

- Be precise. Only extract the text that is directly relevant.
- If the document contains a list of definitions (e.g., lines starting with a bullet point 'â€¢'), extract only the definition that matches the user's question.
- If no specific passage is a good match, return the first paragraph of the document as a general fallback.

USER'S QUESTION: "{query}"

DOCUMENT TEXT:
---
{document_body}
---

EXTRACTED PASSAGE:""",
        input_variables=["query", "document_body"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)



@st.cache_resource
def create_relevance_gate_chain():
    """
    Acts as a final judge. Determines if the top search result is truly
    relevant to the user's query.
    """
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""You are a relevance judge. Your task is to determine if the retrieved document is a good and direct answer to the user's question.

USER'S QUESTION: "{query}"

RETRIEVED DOCUMENT TITLE: "{title}"
RETRIEVED DOCUMENT SNIPPET: "{snippet}"

Based on the title and snippet, is this document a direct and relevant answer to the user's specific question?
Answer with a valid JSON object only, with the keys "is_relevant" (true or false) and "confidence" (a float from 0.0 to 1.0).
{{"is_relevant": true, "confidence": 0.9}}
""",
        input_variables=["query", "title", "snippet"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

# --- NEW: render answer from guide section (+ Solution Image attachments) ---
def render_guide_answer(section: dict, user_query: str = ""):
    """
    Upgraded version: First, uses an LLM to extract the most relevant passage
    from the selected section, then generates a formatted answer from that passage.
    """
    original_body = section.get("body", "")

    try:
        # Create a copy of the section to avoid modifying the original in session state
        modified_section = section.copy()

        # Call the new extractor chain
        extractor_chain = create_llm_passage_extractor_chain()
        extracted_passage = extractor_chain.invoke({
            "query": user_query,
            "document_body": original_body
        })["text"].strip()

        print(f"  -> Extracted Passage: {extracted_passage[:100]}...")

        # Replace the body in our temporary copy with the new, shorter passage
        modified_section["body"] = extracted_passage

        # Generate the final answer using the MODIFIED section
        data = generate_guide_instructions(modified_section, user_query=user_query)

    except Exception as e:
        print(f"  -> Passage extraction failed: {e}. Falling back to full text.")
        # If extraction fails for any reason, fall back to the original behavior
        data = generate_guide_instructions(section, user_query=user_query)

    # --- The rest of the function remains the same ---
    msg = {"role": "assistant", "content": data["answer"], "images": []}
    for p in (data.get("images") or []):
        msg["images"].append(p)
    if msg["content"].strip():
        msg["content"] += f"\n\n> Ø§Ù„Ù…ØµØ¯Ø±: {section.get('title', 'Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³ØªØ§Ø° Ø§Ù„Ø¹Ø§Ù…')}"

    st.session_state.chat_history.append(msg)

    # Reset the error context state
    st.session_state.solution_provided = False
    st.session_state.active_troubleshooting = False
    st.session_state.current_issue = None
    st.session_state.follow_up_attempts = 0
def classify_active_message(user_text: str, kb: list) -> str:
    text = to_english_digits((user_text or "").strip())
    if not (st.session_state.get("solution_provided") and st.session_state.get("current_issue")):
        return "other"
    resolved_cues = ["ØªÙ… Ø­Ù„Ù‡Ø§","Ø§Ù†Ø­Ù„Øª","Ø§Ø´ØªØºÙ„Øª","works now","solved","ØªÙ…Ø§Ù…","Ø´ÙƒØ±Ø§","Ø«Ø§Ù†ÙƒØ³"]
    if any(c in text for c in resolved_cues):
        return "resolved"
    issue = st.session_state.current_issue
    curr_no = str(issue.get("message_number", "")).strip()
    nums = re.findall(r'\d+', text)
    if nums:
        if curr_no and all(n == curr_no for n in nums):
            return "follow_up"
        if curr_no and any(n != curr_no for n in nums):
            return "new_issue"
    followup_cues = ["Ù„Ù… Ø£Ø³ØªØ·Ø¹","Ù„Ù… Ø§Ù‚Ø¯Ø±","Ù…Ø´ ÙˆØ§Ø¶Ø­","ØºÙŠØ± ÙˆØ§Ø¶Ø­","ÙƒÙŠÙ","Ù„Ù… ÙŠØ¹Ù…Ù„","Ù…Ø§ Ø§Ù„Ù…Ù‚ØµÙˆØ¯",
                     "Ù…Ø§ Ø²Ø§Ù„","Ù„Ø§ ÙŠØ²Ø§Ù„","Ø§Ù„Ø®Ø·ÙˆØ©","Ø§Ù„ØªØ§Ù„ÙŠ","Ø£Ø¹Ø¯","Ø£Ø¹Ø¯ Ø§Ù„Ø®Ø·ÙˆØ©","ÙØ³Ù‘Ø±","Ø´Ø±Ø­"]
    if len(text) <= 60 or any(c in text for c in followup_cues):
        return "follow_up"
    try:
        query_emb = get_text_embedding(text)
        curr_emb = get_issue_reference_embedding(issue)
        s_current = util.cos_sim(query_emb, curr_emb).item()
        others = [p for p in kb if p is not issue and 'embedding' in p]
        if others:
            corpus = torch.tensor([p['embedding'] for p in others])
            s_vec = util.cos_sim(query_emb, corpus)[0]
            best_other_idx = int(s_vec.argmax().item())
            s_other = float(s_vec[best_other_idx].item())
        else:
            s_other = 0.0
        if (s_current >= ACTIVE_INTENT_THRESHOLDS["close_to_current"]) and \
           ((s_current - s_other) >= ACTIVE_INTENT_THRESHOLDS["margin"]):
            return "follow_up"
        if s_other >= ACTIVE_INTENT_THRESHOLDS["new_issue_candidate"] and s_other > s_current:
            return "new_issue"
        return "follow_up"
    except Exception:
        return "follow_up"

def semantic_search(query: str):
    kb = st.session_state.get('knowledge_base', [])
    if not kb:
        return {"answer": "Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙØ§Ø±ØºØ©."}

    # 1) number-first routing
    query = to_english_digits(query or "")
    query_numbers = re.findall(r'\d+', query)
    if query_numbers:
        # exact number in KB?
        for problem in kb:
            kb_number = str(problem.get('message_number', '')).strip()
            if kb_number and kb_number in query_numbers:
                return problem
        # number mentioned but not found â†’ ask a human
        return {"not_found": True, "reason": "number_not_in_kb", "number": query_numbers[0]}

    # 2) semantic + lexical gating
    embedding_model = get_embedding_model()
    corpus_embeddings = torch.tensor([p['embedding'] for p in kb])
    query_embedding = embedding_model.encode(query, convert_to_tensor=True)

    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    best_idx = int(torch.argmax(cos_scores).item())
    best_score = float(cos_scores[best_idx].item())
    best_match = kb[best_idx]

    # lexical signals against the best candidate
    title = (best_match.get("message_text") or "")[:300]
    lex = _lexical_ratio(query, title)
    cov = _token_coverage(query, title)

    # thresholds (tuned conservatively)
    SEM_ACCEPT = 0.66     # accept as direct answer above this
    SEM_SUGGEST = 0.58    # consider "did you mean?" above this
    LEX_ACCEPT = 0.80     # strong lexical
    LEX_SUGGEST = 0.70    # weaker but still plausible
    COV_ACCEPT = 0.80     # token overlap coverage
    COV_SUGGEST = 0.60

    # Accept only if clearly close on either semantic or lexical
    if best_score >= SEM_ACCEPT or lex >= LEX_ACCEPT or cov >= COV_ACCEPT:
        return best_match

    # Offer "did you mean?" only when thereâ€™s some convincing overlap
    if (best_score >= SEM_SUGGEST) and (lex >= LEX_SUGGEST or cov >= COV_SUGGEST):
        return {"suggestion": best_match}

    # Otherwise, don't risk a wrong answer â†’ handoff
    return {"not_found": True, "reason": "low_similarity"}


def log_user_feedback(query: str, chosen_title: str, feedback_file: str = "feedback_log.csv"):
    """Appends a query and the user's chosen correct answer to a CSV log file."""
    try:
        # Create file and header if it doesn't exist
        if not os.path.exists(feedback_file):
            with open(feedback_file, "w", newline="", encoding="utf-8-sig") as f:
                writer = csv.writer(f)
                writer.writerow(["timestamp_utc", "query", "chosen_title"])

        # Append the new feedback
        with open(feedback_file, "a", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            writer.writerow([datetime.now(timezone.utc).isoformat(), query, chosen_title])
        print(f"Feedback logged: '{query}' -> '{chosen_title}'")
    except Exception as e:
        print(f"Error logging feedback: {e}")


def _ensure_kb_matrices():
    # Build tensors once (for speed) after kb loads
    if st.session_state.get("gl_errors_kb") and "gl_errors_matrix" not in st.session_state:
        st.session_state.gl_errors_matrix = torch.tensor([p["embedding"] for p in st.session_state.gl_errors_kb])
    if st.session_state.get("gl_guide_kb") and "gl_guide_matrix" not in st.session_state:
        st.session_state.gl_guide_matrix = torch.tensor([s["embedding"] for s in st.session_state.gl_guide_kb])


def _lexical_ratio(a: str, b: str) -> float:
    from difflib import SequenceMatcher
    def normalize(s: str) -> str:
        s = (s or "").lower()
        s = re.sub(r"[^\w\s\u0600-\u06FF]", " ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    return SequenceMatcher(None, normalize(a), normalize(b)).ratio()


from langchain_google_genai import \
    GoogleGenerativeAIEmbeddings  # <-- Make sure to add this import at the top of your file





# @st.cache_resource
# def create_keyword_extraction_chain():
#     """
#     Creates an LLM chain that extracts key technical nouns from a user's query.
#     """
#     llm = get_llm_model()
#     prompt = PromptTemplate(
#         template="""You are an expert search pre-processor. Your task is to extract the key nouns and technical terms from the user's query. Ignore conversational words like 'how to', 'what is', 'explain', etc. Respond with only the extracted terms.
#
# User Query: "{query}"
#
# Example 1:
# User Query: "ÙƒÙŠÙ Ø§Ø¶ÙŠÙ Ø³Ù†Ø¯ Ù‚Ø¨Ø¶ Ù„Ø¹Ù…ÙŠÙ„"
# Extracted Terms: "Ø³Ù†Ø¯ Ù‚Ø¨Ø¶ Ø¹Ù…ÙŠÙ„"
#
# Example 2:
# User Query: "Ù…Ø§ Ù‡Ùˆ ØªÙ‚Ø±ÙŠØ± Ù…ÙŠØ²Ø§Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©"
# Extracted Terms: "ØªÙ‚Ø±ÙŠØ± Ù…ÙŠØ²Ø§Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©"
#
# Now, extract the key terms from the user's query above.
# Extracted Terms:""",
#         input_variables=["query"],
#     )
#     return LLMChain(llm=llm, prompt=prompt, verbose=False)




def llm_search_gl_guide(query: str, k_titles: int = 12, top_m: int = 3):
    """
    Pure LLM retrieval:
    - Round 1: feed ALL titles to LLM â†’ pick up to k_titles ids.
    - Round 2: feed snippets of those to LLM â†’ get top_m ranked ids.
    - Return same shape your UI expects: {"candidates": [sections...]}
    """
    kb = st.session_state.get("gl_guide_kb", []) or []
    if not kb:
        return {"answer": "Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (GL) ØºÙŠØ± Ù…Ø­Ù…Ù‘Ù„ Ø¨Ø¹Ø¯."}

    # ---- Round 1: titles only ----
    lines = []
    for i, sec in enumerate(kb):
        title = (sec.get("title") or "").strip().replace("\n", " ")
        cat   = (sec.get("category") or "").strip()
        lines.append(f"{i} | {title} | {cat}")
    items_blob = "\n".join(lines)

    sel_chain = create_llm_title_selector_chain()
    try:
        r1_raw = sel_chain.invoke({"query": query, "items": items_blob, "k": k_titles})["text"]
        r1 = _json_only(r1_raw)
        ids1 = [int(x) for x in (r1.get("ids") or []) if 0 <= int(x) < len(kb)]
    except Exception:
        # fallback: naive lexical pick of first 12
        ids1 = list(range(min(k_titles, len(kb))))

    if not ids1:
        return {"answer": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø³Ù… Ù…Ø·Ø§Ø¨Ù‚."}

    # ---- Round 2: snippets for the selected ----
    cand_lines = []
    for i in ids1:
        sec = kb[i]
        title = (sec.get("title") or "").strip().replace("\n", " ")
        cat   = (sec.get("category") or "").strip()
        body  = (sec.get("body") or "").strip().replace("\r", " ")
        snippet = body[:480].replace("\n", " ")
        cand_lines.append(f"{i} | {title} | {cat} | {snippet}")
    cands_blob = "\n".join(cand_lines)

    rank_chain = create_llm_candidate_ranker_chain()
    try:
        r2_raw = rank_chain.invoke({"query": query, "cands": cands_blob})["text"]
        r2 = _json_only(r2_raw)
        ranked = [int(x) for x in (r2.get("ranked_ids") or []) if x in ids1]
    except Exception:
        ranked = ids1[:top_m]

    if not ranked:
        ranked = ids1[:top_m]

    # keep top_m sections
    out = [kb[i] for i in ranked[:top_m]]
    return {"candidates": out}

@st.cache_resource
def create_llm_errors_selector_chain():
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""
Ø§Ø®ØªØ± Ø£ÙØ¶Ù„ Ø¹Ù†ØµØ± Ø±Ø³Ø§Ù„Ø© Ø®Ø·Ø£ ÙŠØ·Ø§Ø¨Ù‚ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
Ø£ÙˆÙ„ÙˆÙŠØ© Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø±Ù‚Ù… Ø¥Ù† ÙˆÙØ¬Ø¯ ØµØ±Ø§Ø­Ø©Ù‹.

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
{query}

Ø§Ù„Ø¹Ù†Ø§ØµØ± (id | message_number | message_text | reason):
{items}

JSON ÙÙ‚Ø·:
{{"best_id": <int or -1>, "confidence": 0.0_to_1.0}}
""",
        input_variables=["query","items"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

def search_gl_errors_llm(query: str):

    kb = st.session_state.get("gl_errors_kb", []) or []
    if not kb:
        return {"answer":"Ù…Ù„Ù Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ØºÙŠØ± Ù…Ø­Ù…Ù‘Ù„ Ø¨Ø¹Ø¯."}

    # prefer IDs detected by the smart extractor
    detected = st.session_state.pop("__detected_error_ids__", [])
    if detected:
        for n in detected:
            for p in kb:
                if str(p.get("message_number", "")).strip() == n:
                    return p

    q = to_english_digits(query or "")
    # 1) exact number short-circuit
    nums = re.findall(r"\d{3,}", q)
    if nums:
        nset = set(nums)
        for i,p in enumerate(kb):
            if str(p.get("message_number","")).strip() in nset:
                return p

    # 2) LLM shortlist
    items = "\n".join(
        f"{i} | {p.get('message_number','')} | {(p.get('message_text') or '').strip()} | {(p.get('reason') or '').strip()}"
        for i,p in enumerate(kb)
    )
    raw = create_llm_errors_selector_chain().invoke({"query": query, "items": items})["text"]
    data = _json_only(raw) or {}
    i = int(data.get("best_id", -1))
    return kb[i] if 0 <= i < len(kb) else {"not_found": True, "reason": "no_match"}




@st.cache_resource
def create_intent_category_chain():
    """
    Creates an LLM chain that classifies the user's query into one of the four
    business categories.
    """
    llm = get_llm_model()
    prompt = PromptTemplate(
        template="""You are an expert ERP system router. Your job is to classify the user's query into one of four categories based on their likely intent. The categories are:
- Configuration: For system administrators setting up core system variables. High-stakes, foundational questions.
- Inputs: For users creating or defining master data (e.g., "how to add a bank", "create a cash fund").
- Operations: For users performing a financial transaction (e.g., "issue a receipt voucher", "reconcile a bank account").
- Reports: For managers or users analyzing data and outcomes (e.g., "show me the trial balance", "what is the profit and loss report?").

User Query: "{query}"

Based on the user's query, which of the four categories is the most likely target?
Respond with JSON only, like this: {{"category": "<Configuration|Inputs|Operations|Reports>"}}""",
        input_variables=["query"],
    )
    return LLMChain(llm=llm, prompt=prompt, verbose=False)

def initialize_session_state():
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "kb_loaded" not in st.session_state:
        st.session_state.kb_loaded = False
    if "rejection_count" not in st.session_state:
        st.session_state.rejection_count = 0

    if "is_in_rephrase_loop" not in st.session_state:
        st.session_state.is_in_rephrase_loop = False
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferMemory(
            memory_key="chat_history",
            input_key="question",
            return_messages=False
        )

    # Placeholders
    if "gl_guide_kb" not in st.session_state:
        st.session_state.gl_guide_kb = []
    if "gl_errors_kb" not in st.session_state:
        st.session_state.gl_errors_kb = []



    # âœ… Load all KBs (legacy â†’ guide â†’ errors). Errors last to set valid_message_numbers.
    legacy_ok = load_knowledge_base()
    guide_ok  = load_gl_guide_kb()

    _ensure_kb_matrices()
    build_gl_dyk_index()
    errors_ok = load_errors_kb()

    st.session_state.kb_loaded = legacy_ok or guide_ok or errors_ok or st.session_state.kb_loaded

    if "processed_image_id" not in st.session_state:
        st.session_state.processed_image_id = None
    if "awaiting_more" not in st.session_state:
        st.session_state.awaiting_more = False
    if "current_issue" not in st.session_state:
        st.session_state.current_issue = None
    if "current_issue_embedding" not in st.session_state:
        st.session_state.current_issue_embedding = None
    if "active_troubleshooting" not in st.session_state:
        st.session_state.active_troubleshooting = False
    if "awaiting_anything_else" not in st.session_state:
        st.session_state.awaiting_anything_else = False
    if "solution_provided" not in st.session_state:
        st.session_state.solution_provided = False
    if "follow_up_attempts" not in st.session_state:
        st.session_state.follow_up_attempts = 0
    if "pending_suggestion" not in st.session_state:
        st.session_state.pending_suggestion = None
    if "pending_choices" not in st.session_state:
        st.session_state.pending_choices = None

@st.cache_resource
def create_followup_policy_chain():
    llm = get_llm_model()
    policy_prompt = """
Ø£Ù†Øª "Ù…Ø´Ø±Ù Ù…ØªØ§Ø¨Ø¹Ø©" Ø°ÙƒÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯ Onyx ERP.
Ù„Ø¯ÙŠÙƒ:
- Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: Ø±Ù‚Ù… {message_number} / Ù†Øµ: {message_text}
- Ø§Ù„Ø³Ø¨Ø¨: {reason}
- Ø§Ù„Ø­Ù„Ù‘ Ø§Ù„ÙƒØ§Ù…Ù„ (Ù…ØµØ¯Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¥ØµÙ„Ø§Ø­): {solution}
- Ø¢Ø®Ø± Ø±Ø¯Ù‘ÙÙŠÙ† Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯: [1] {last_assistant_1}  |  [2] {last_assistant_2}
- Ø¢Ø®Ø± Ø±Ø¯Ù‘ÙÙŠÙ† Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…: [1] {last_user_1}  |  [2] {last_user_2}
- Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©: {user_text}

Ù…Ù‡Ù…ØªÙƒ: Ù‚Ø±Ù‘ÙØ± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… (Ø¯ÙˆÙ† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ø¹Ø¯ Ø®Ø§ØµØ© Ø¨ÙƒÙ„ Ø®Ø·Ø£):
- "step_help": ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… **Ø®Ø·ÙˆØ© Ø¬Ø¯ÙŠØ¯Ø© ÙˆÙ…Ù…ÙŠÙ‘Ø²Ø© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ°** Ù…Ø³ØªØ®Ù„ØµØ© Ù…Ù† Ø§Ù„Ø­Ù„ Ø£Ø¹Ù„Ø§Ù‡ØŒ ÙˆÙ„Ù… ØªÙØ°ÙƒØ± ÙÙŠ Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø£Ø®ÙŠØ±Ø©.
- "ask_clarify": ØªØ­ØªØ§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø© Ù…Ø­Ø¯Ø¯Ø© Ù…ÙÙ‚ÙˆØ¯Ø© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© (Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ù‹Ø§ ÙˆØ§Ø­Ø¯Ù‹Ø§ Ø¯Ù‚ÙŠÙ‚Ù‹Ø§ ÙÙ‚Ø·).
- "escalate": Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ¨Ø¯Ùˆ Ù…Ø­Ø¬ÙˆØ¨Ù‹Ø§/Ù…Ù‚ÙŠÙ‘Ø¯ ØµÙ„Ø§Ø­ÙŠØ§Øª/Ù„Ø§ ÙŠØµÙ„ Ù„Ù„Ø´Ø§Ø´Ø© Ø§Ù„Ù„Ø§Ø²Ù…Ø©/Ø¬Ø±Ù‘ÙØ¨ ÙˆÙ„Ù… ÙŠÙ†Ø¬Ø­/Ø£Ùˆ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø®Ø·ÙˆØ© Ø¬Ø¯ÙŠØ¯Ø© ÙˆØ§Ø¶Ø­Ø© Ù…Ù† Ø§Ù„Ø­Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§ Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø±. ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø© ÙŠØ¬Ø¨ Ø§Ù„ØªØµØ¹ÙŠØ¯ ÙÙˆØ±Ù‹Ø§ Ù„Ù…Ø³ØªØ´Ø§Ø±.

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø·:
{{
  "action": "step_help" | "ask_clarify" | "escalate",
  "assistant_reply": "<Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ Ø³Ø£Ù‚ÙˆÙ„Ù‡ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ø°Ø§ ÙƒØ§Ù†Øª action Ù„Ø§ ØªØ³Ø§ÙˆÙŠ escalate>",
  "confidence": 0.0
}}
"""
    return LLMChain(
        llm=llm,
        prompt=PromptTemplate(
            template=policy_prompt,
            input_variables=[
                "message_number","message_text","reason","solution",
                "last_assistant_1","last_assistant_2","last_user_1","last_user_2",
                "user_text"
            ]
        ),
        verbose=False
    )

@st.cache_resource
def create_conversational_chain():
    llm = get_llm_model()
    prompt_template = """
    Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¯Ø¹Ù… ÙÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙ‚Ø· ÙÙŠ Ù†Ø¸Ø§Ù… "Onyx ERP". Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙØ´Ù„ Ø¨Ø­Ø«Ù‡Ù… Ø§Ù„Ø£ÙˆÙ„ÙŠ.
    Ø§ØªØ¨Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµØ§Ø±Ù…Ø©:
    1.  **Ù„Ø§ ØªÙƒÙ† Ø¹Ø§Ù…Ù‹Ø§ Ø£Ø¨Ø¯Ù‹Ø§**: Ù„Ø§ ØªØ³Ø£Ù„ Ø¹Ù† "Ù†ÙˆØ¹ Ø§Ù„Ø¬Ù‡Ø§Ø²" Ø£Ùˆ "Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ´ØºÙŠÙ„".
    2.  **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØºØ§Ù…Ø¶Ù‹Ø§**: Ø§Ø·Ù„Ø¨ Ù…Ù†Ù‡ Ø¨Ø£Ø¯Ø¨ ØªÙ‚Ø¯ÙŠÙ… ÙˆØµÙ Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ù„Ù„Ù…Ø´ÙƒÙ„Ø© Ø£Ùˆ Ø±Ù‚Ù… Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£.
    3.  **Ø¥Ø°Ø§ Ù‚Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ù† Ø§Ù„Ø­Ù„ Ù„Ù… ÙŠÙ†Ø¬Ø­**: Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ ØªÙˆØ¶ÙŠØ­ÙŠÙ‹Ø§ Ø­ÙˆÙ„ Ø§Ù„Ø­Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚.
    4.  **ÙƒÙ† Ù…ÙˆØ¬Ø²Ù‹Ø§ ÙˆÙ…Ø¨Ø§Ø´Ø±Ù‹Ø§**.
    ---
    Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
    Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ: {question}
    Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø§Ù„Ù…Ø±ÙƒØ²Ø© ÙˆØ§Ù„Ù…Ø®ØªØµØ±Ø©:
    """
    prompt = PromptTemplate(template=prompt_template, input_variables=["chat_history", "question"])
    return LLMChain(llm=llm, prompt=prompt, memory=st.session_state.memory, verbose=True)

def render_sidebar():
    with st.sidebar:
        st.file_uploader("Ø£Ùˆ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£", type=["png", "jpg", "jpeg"], key="image_uploader")
        st.markdown("---")
        with st.expander("ğŸ§  Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£Ø¯Ù„Ø© (GL ÙÙ‚Ø· Ø§Ù„Ø¢Ù†)"):
            st.caption("Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ø§ Ù„Ù… ØªØºÙŠÙ‘ÙØ± Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª:")
            guide_pdf = st.text_input("Ù…Ø³Ø§Ø± Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (GL)", GUIDE_PDF_DEFAULT)
            errors_pdf = st.text_input("Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ø­Ù„ÙˆÙ„", ERRORS_PDF_DEFAULT)

            colA, colB = st.columns(2)
            if colA.button("Ø¨Ù†Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (GL)", type="primary"):
                if not os.path.exists(guide_pdf):
                    st.error(f"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {guide_pdf}")
                elif build_gl_guide_kb(guide_pdf):
                    load_gl_guide_kb()
                    st.success("ØªÙ… Ø¨Ù†Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (GL).")

            if colB.button("Ø¨Ù†Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ (GL)"):
                if not os.path.exists(errors_pdf):
                    st.error(f"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {errors_pdf}")
                elif build_errors_kb(errors_pdf):
                    load_errors_kb()
                    st.success("ØªÙ… Ø¨Ù†Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ (GL).")

def render_welcome_screen():
    st.title("ğŸ¤– Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ")
    st.markdown("### Ù„Ù„Ø¨Ø¯Ø¡ØŒ ÙŠØ±Ø¬Ù‰ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø£ÙˆÙ„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")


def render_suggestion_buttons():
    suggestion_data = st.session_state.pending_suggestion

    if "message_text" in suggestion_data:
        suggestion_text = suggestion_data['message_text']
    elif "title" in suggestion_data:
        suggestion_text = suggestion_data['title']
    else:
        suggestion_text = "Ø§Ù‚ØªØ±Ø§Ø­ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"

    st.info(f'Ù‡Ù„ ØªÙ‚ØµØ¯ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŸ "{suggestion_text}"')

    col1, col2 = st.columns(2)
    if col1.button("Ù†Ø¹Ù…", key="confirm_suggestion", use_container_width=True):
        confirmed_item = st.session_state.pending_suggestion
        st.session_state.pending_suggestion = None

        if "message_number" in confirmed_item:
            handle_confirmed_solution(confirmed_item)
        elif "body" in confirmed_item:
            last_user_query = ""
            for msg in reversed(st.session_state.chat_history):
                if msg["role"] == "user":
                    last_user_query = msg["content"]
                    break
            render_guide_answer(confirmed_item, user_query=last_user_query)
        st.rerun()

    if col2.button("Ù„Ø§", key="reject_suggestion", use_container_width=True):
        st.session_state.pending_suggestion = None
        st.session_state.chat_history.append(
            {"role": "assistant", "content": "Ø­Ø³Ù†Ù‹Ø§ØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ÙˆØµÙ Ù…Ø§ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡ Ø¨Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØµÙŠÙ„ØŸ"})
        st.rerun()


def render_choice_buttons():
    """
    Renders choices and logs the user's selection for future training.
    """
    st.info("Ù„Ù‚Ø¯ ÙˆØ¬Ø¯Øª Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø°Ø§Øª ØµÙ„Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ù†Ø³Ø¨:")

    candidate_sections = st.session_state.pending_choices

    for i, section in enumerate(candidate_sections):
        if st.button(section['title'], key=f"choice_{i}", use_container_width=True):
            last_user_query = ""
            for msg in reversed(st.session_state.chat_history):
                if msg["role"] == "user":
                    last_user_query = msg["content"]
                    break


            st.session_state.pending_choices = None
            render_guide_answer(section, user_query=last_user_query)
            st.rerun()

    if st.button("Ù„Ø§ Ø´ÙŠØ¡ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª", key="reject_choices", use_container_width=True):
        st.session_state.pending_choices = None
        st.session_state.rejection_count += 1  # Increment the counter

        if st.session_state.rejection_count >= 2:
            print("Search failed twice. Escalating to advisor.")

            # Find the last user query that caused the failure
            last_user_query = ""
            for msg in reversed(st.session_state.chat_history):
                if msg["role"] == "user":
                    last_user_query = msg["content"]
                    break

            _escalate_and_reset(last_user_query, preface="Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….")
            return

        else:
            st.session_state.chat_history.append(
                {"role": "assistant", "content": "Ø­Ø³Ù†Ù‹Ø§ØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ÙˆØµÙ Ù…Ø§ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ø¨Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØµÙŠÙ„ØŸ"})

        st.rerun()

def render_chat_interface():
    """Renders the main chat UI and handles the interaction logic."""
    st.title("Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù†Ø¸Ø§Ù… Onyx ERP")

    if not st.session_state.chat_history:
        st.markdown("<div style='text-align: center;'>Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø°ÙƒÙŠ Ù„Ø­Ù„ Ù…Ø´Ø§ÙƒÙ„ Ù†Ø¸Ø§Ù… Onyx ERP.</div>", unsafe_allow_html=True)
        st.markdown("<div style='text-align: center; color: grey;'>ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ø®ØªÙŠØ§Ø± Ø£Ø­Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø£Ø¯Ù†Ø§Ù‡.</div><br>", unsafe_allow_html=True)

        cols = st.columns([1, 1, 1])
        if cols[0].button("Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© 'ØµÙ†Ù Ù…Ù†ØªÙ‡ÙŠ' (1495)", use_container_width=True):
            handle_text_query("1495"); st.rerun()
        if cols[1].button("Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© 'ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª' (669)", use_container_width=True):
            handle_text_query("669"); st.rerun()
        if cols[2].button("Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© 'ÙƒÙ…ÙŠØ§Øª Ù…Ø¬Ø§Ù†ÙŠØ©' (939)", use_container_width=True):
            handle_text_query("939"); st.rerun()

    for msg in st.session_state.chat_history:
        avatar = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"], unsafe_allow_html=True)
            if "images" in msg and msg.get("images"):
                for img_data in msg["images"]:
                    st.image(img_data, use_container_width=True)

    chat_input_disabled = bool(st.session_state.pending_suggestion)

    if st.session_state.pending_suggestion:
        render_suggestion_buttons()
    elif st.session_state.pending_choices:
        render_choice_buttons()

    chat_input_disabled = bool(st.session_state.pending_suggestion or st.session_state.pending_choices)

    if st.session_state.solution_provided:
        if st.button("âœ… ØªÙ… Ø­Ù„Ù‡Ø§", use_container_width=True):
            st.session_state.solution_provided = False
            st.session_state.follow_up_attempts = 0
            st.session_state.active_troubleshooting = False
            st.session_state.current_issue = None
            st.session_state.current_issue_embedding = None
            st.session_state.chat_history.append({"role": "assistant", "content": "Ø±Ø§Ø¦Ø¹! ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ø£Ù†Ù†ÙŠ ØªÙ…ÙƒÙ†Øª Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©."})
            st.rerun()

    if st.session_state.get("image_uploader") and st.session_state.image_uploader.file_id != st.session_state.processed_image_id:
        handle_image_upload()

    if user_query := st.chat_input("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø°ÙŠ ØªÙˆØ§Ø¬Ù‡Ù‡ØŸ", disabled=chat_input_disabled):
        handle_text_query(user_query)

def handle_image_upload():
    uploaded_image = st.session_state.image_uploader
    st.session_state.processed_image_id = uploaded_image.file_id
    image_bytes = uploaded_image.getvalue()
    st.session_state.chat_history.append({"role": "user", "content": f"ØªÙ… ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø©: `{uploaded_image.name}`", "images": [image_bytes]})

    if st.session_state.solution_provided:
        interception_message = """
        Ù„Ø­Ø¸Ø© Ù…Ù† ÙØ¶Ù„ÙƒØŒ ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ù†Ø§ Ù…Ø§ Ø²Ù„Ù†Ø§ Ù†Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.

        - Ø¥Ø°Ø§ ØªÙ… Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± **"âœ… ØªÙ… Ø­Ù„Ù‡Ø§"** Ø§Ù„Ø£Ø®Ø¶Ø±.
        - Ø¥Ø°Ø§ ÙƒÙ†Øª Ù„Ø§ ØªØ²Ø§Ù„ ØªÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø©ØŒ Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø­ØªÙ‰ Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø£Ùˆ ØªÙˆØ¬ÙŠÙ‡Ùƒ Ø¥Ù„Ù‰ Ù…Ø³ØªØ´Ø§Ø±.
        """
        st.session_state.chat_history.append({"role": "assistant", "content": interception_message})
        st.rerun()
        return

    with st.spinner("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©..."):
        extracted_number = llm_extract_message_number_from_image(image_bytes)

    if extracted_number:
        handle_text_query(extracted_number)
    else:
        st.session_state.chat_history.append({"role": "assistant",
                                              "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ù‚Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©. Ø­Ø§ÙˆÙ„ ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙƒØªØ§Ø¨ÙŠØ§Ù‹."})
        st.rerun()

def handle_confirmed_solution(problem_data):
    st.session_state.solution_provided = True
    st.session_state.follow_up_attempts = 0
    response_data = generate_instructional_response(problem_data)

    st.session_state.current_issue = problem_data
    st.session_state.current_issue_embedding = None
    st.session_state.active_troubleshooting = True

    st.session_state.chat_history.append({
        "role": "assistant",
        "content": response_data.get("answer"),
        "images": response_data.get("images", [])
    })
    st.session_state.memory.save_context(
        {"question": problem_data['message_text']},
        {"output": response_data.get("answer", "")}
    )

def handle_text_query(user_query):
    """
    Search -> Format -> Converse flow with:
    - post-close guard ("anything else?" flow) + open_new intent
    - hard handoff if the user explicitly asks for a human
    - PERMISSION hotword override -> escalate with preface
    - active-issue router (follow_up/new_issue/open_new/resolved/other)
    - "did you mean?" confirmation for partial titles (<95% lexical match)
    - immediate escalation if user typed a non-existent error number (errors-only)
    - NEW: guide/errors source routing + attach GL guide 'Solution Image' pictures
    """
    import difflib
    st.session_state.chat_history.append({"role": "user", "content": user_query})

    # ---------------- helpers ----------------
    def _norm(s: str) -> str:
        return (s or "").strip().lower()

    def _normalize_for_title(s: str) -> str:
        s = _norm(s)
        s = re.sub(r"[^\w\s\u0600-\u06FF]", " ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def _lexical_ratio(a: str, b: str) -> float:
        return difflib.SequenceMatcher(None, _normalize_for_title(a), _normalize_for_title(b)).ratio()

    def _looks_like_error_intent(s: str) -> bool:
        t = (s or "").strip().lower()
        if re.fullmatch(r"\d{3,}", re.sub(r"\s+", "", to_english_digits(t))):
            return True
        if re.search(r"\b(error|code|message|msg|Ø±Ø³Ø§Ù„Ø©|Ø®Ø·Ø£|Ù…Ø´ÙƒÙ„Ø©)\b", t):
            return True
        return bool(re.search(r"\d{3,}", t))

    def _simple_open_new(s: str) -> bool:
        t = _norm(s)
        return bool(re.search(r"(Ø§ÙØªØ­|ÙØªØ­|Ø§Ø¨Ø¯Ø£|Ø¨Ø¯Ø¡)\s+(Ù…Ø´ÙƒÙ„Ø©|ØªØ°ÙƒØ±Ø©)|open\s+new\s+(issue|ticket)", t))

    # ===== 0) GLOBAL PERMISSION OVERRIDE =====
    if _is_permission_related({}, user_query):
        preface = (
            "ğŸ”’ **Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª**.\n"
            "Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† ØªÙ…ØªÙ„Ùƒ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ø±Ø¬Ø§Ø¡Ù‹ ØªÙˆØ§ØµÙ„ Ù…Ø¹ **Ù…Ø³Ø¤ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…** Ø£Ùˆ **Ù…Ø¯ÙŠØ±Ùƒ** "
            "Ù„Ù…Ù†Ø­Ùƒ Ø§Ù„Ø¥Ø°Ù† Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø©/Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨.\n"
            "Ø³Ø£Ù‚ÙˆÙ… Ø§Ù„Ø¢Ù† Ø¨Ø¥ØºÙ„Ø§Ù‚ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© (Ø¥Ù† ÙˆÙØ¬Ø¯Øª) ÙˆØªØ­ÙˆÙŠÙ„Ùƒ Ø¥Ù„Ù‰ Ù…Ø³ØªØ´Ø§Ø± Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ù†Ø­ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©."
        )
        _escalate_and_reset(user_query, preface=preface)
        return

    # ===== 1) HARD HANDOFF: explicit ask for human =====
    if _wants_handoff(user_query):
        _escalate_and_reset(user_query)
        return

    # ===== 2) POST-CLOSE GUARD ("anything else?" mode) =====
    if st.session_state.get("awaiting_anything_else"):
        router = create_intent_router_chain()
        ctx = {
            "current_number": "",
            "current_text": "",
            "current_reason": "",
            "current_solution": "",
            "last_assistant_msg": _last_assistant_message_text(),
            "valid_numbers": _numbers_catalog(),
            "user_text": user_query,
        }
        try:
            decision_raw = router.invoke(ctx)["text"]
            decision = _json_from_model_text(decision_raw)
        except Exception:
            decision = {"intent": "other", "new_issue_number": None, "confidence": 0.4}

        intent = decision.get("intent", "other")
        new_no = decision.get("new_issue_number")

        if intent == "other" and _simple_open_new(user_query):
            intent = "open_new"

        if intent == "open_new":
            st.session_state.awaiting_anything_else = False
            reply = "ØªÙ… ğŸ‘ â€” Ø£Ø±Ø³Ù„ Ø±Ù‚Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø£Ùˆ ÙˆØµÙÙ‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ Ù„Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©."
            st.session_state.chat_history.append({"role": "assistant", "content": reply})
            st.session_state.memory.save_context({"question": user_query}, {"output": reply})
            return st.rerun()

        if intent == "new_issue" and new_no:
            st.session_state.awaiting_anything_else = False
            with st.spinner("ğŸ” Ø¬Ø§Ø±ÙŠ ÙØªØ­ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©..."):
                sr = search_gl_errors_llm(str(new_no))
                if "message_number" in sr:
                    st.session_state.solution_provided = True
                    st.session_state.follow_up_attempts = 0
                    st.session_state.current_issue = sr

                    st.session_state.active_troubleshooting = True
                    resp = generate_instructional_response(sr)
                    st.session_state.chat_history.append({
                        "role": "assistant",
                        "content": resp.get("answer"),
                        "images": resp.get("images", [])
                    })
                    st.session_state.memory.save_context({"question": user_query}, {"output": resp.get("answer", "")})
                    return st.rerun()
                else:
                    st.session_state.chat_history.append({"role": "assistant",
                        "content": "Ù„Ù… Ø£Ø¬Ø¯ Ø±Ù‚Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„. Ø³Ø£ÙˆØµÙ„Ùƒ Ø¨Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ø¢Ù†."})
                    _escalate_and_reset(user_query)
                    return

        reply = "Ù‡Ù„ ØªØ±ÙŠØ¯ ÙØªØ­ Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø£Ù… ÙƒÙ„ Ø´ÙŠØ¡ ØªÙ…Ø§Ù… Ø§Ù„Ø¢Ù†ØŸ"
        st.session_state.chat_history.append({"role": "assistant", "content": reply})
        st.session_state.memory.save_context({"question": user_query}, {"output": reply})
        return st.rerun()

    # ===== 3) ACTIVE-ISSUE BRANCH =====
    if st.session_state.get("solution_provided"):
        kb = st.session_state.get("knowledge_base", []) or []
        issue = st.session_state.get("current_issue") or {}
        router = create_intent_router_chain()
        ctx = {
            "current_number": str(issue.get("message_number", "")),
            "current_text": (issue.get("message_text", "") or "")[:800],
            "current_reason": (issue.get("reason", "") or "")[:600],
            "current_solution": (issue.get("solution", "") or "")[:1200],
            "last_assistant_msg": _last_assistant_message_text(),
            "valid_numbers": _numbers_catalog(),
            "user_text": user_query
        }
        try:
            decision_raw = router.invoke(ctx)["text"]
            decision = _json_from_model_text(decision_raw)
        except Exception:
            decision = {"intent": "follow_up", "step_number": None, "new_issue_number": None, "confidence": 0.5}

        intent = decision.get("intent", "other")
        new_no = decision.get("new_issue_number")
        step_no = decision.get("step_number")

        if intent == "other" and _simple_open_new(user_query):
            intent = "open_new"

        if intent == "resolved":
            st.session_state.solution_provided = False
            st.session_state.follow_up_attempts = 0
            st.session_state.active_troubleshooting = False
            st.session_state.current_issue = None
            st.session_state.current_issue_embedding = None
            st.session_state.awaiting_anything_else = True
            st.session_state.chat_history.append({"role": "assistant", "content": "Ø±Ø§Ø¦Ø¹! ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø´ÙŠØ¦Ù‹Ø§ Ø¢Ø®Ø±ØŸ"})
            return st.rerun()

        if intent == "new_issue" and new_no:
            st.session_state.chat_history.append({"role": "assistant", "content":
                f"Ø°ÙƒØ±Øª Ø±Ù‚Ù… Ø±Ø³Ø§Ù„Ø© Ù…Ø®ØªÙ„Ù **{new_no}**. Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø§Ø¶ØºØ· **\"âœ… ØªÙ… Ø­Ù„Ù‡Ø§\"** Ø«Ù… Ø£Ø±Ø³Ù„ Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„Ù†ÙØªØ­Ù‡Ø§."})
            return st.rerun()

        if intent == "open_new":
            st.session_state.awaiting_anything_else = True
            reply = (
                "Ù„Ø­Ø¸Ø© Ù…Ù† ÙØ¶Ù„Ùƒ â€” Ù…Ø§ Ø²Ù„Ù†Ø§ Ù†Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.\n"
                "â€¢ Ø¥Ø°Ø§ ØªÙ… Ø­Ù„Ù‡Ø§ØŒ Ø§Ø¶ØºØ· Ø²Ø± **\"âœ… ØªÙ… Ø­Ù„Ù‡Ø§\"** Ø§Ù„Ø£Ø®Ø¶Ø±.\n"
                "â€¢ Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª ÙØªØ­ Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ø¢Ù†ØŒ Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø¨Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø£Ùˆ Ø£Ø±Ø³Ù„ **Ø±Ù‚Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø©/ÙˆØµÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©**."
            )
            st.session_state.chat_history.append({"role": "assistant", "content": reply})
            st.session_state.memory.save_context({"question": user_query}, {"output": reply})
            return st.rerun()

        if intent == "follow_up":

            st.session_state.active_troubleshooting = True
            st.session_state.follow_up_attempts += 1

            # --- NEW ROUTER LOGIC ---
            print("--- ğŸ§  Follow-up Router Initiated ---")
            followup_router = create_followup_router_chain()
            solution_context = (issue.get("solution") or "")[:1000]  # Provide context from the solution

            try:
                router_raw = followup_router.invoke({
                    "solution_context": solution_context,
                    "query": user_query
                })["text"]
                followup_intent = _json_only(router_raw).get("intent", "troubleshoot_error")
                print(f"  -> Follow-up intent detected: {followup_intent}")
            except Exception as e:
                print(f"  -> Follow-up router failed: {e}. Defaulting to troubleshoot_error.")
                followup_intent = "troubleshoot_error"

            if followup_intent == "search_guide":
                # Path B: The user asked a "how-to" question. Search the guide.
                with st.spinner("ğŸ” Ø¨Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…..."):
                    res = llm_search_gl_guide(user_query)
                    if "candidates" in res and res["candidates"]:
                        best_choice = res["candidates"][0]
                        render_guide_answer(best_choice, user_query)
                        st.session_state.follow_up_attempts -= 1  # This successful lookup doesn't count as a "failed" attempt
                    else:
                        st.session_state.chat_history.append({"role": "assistant",
                                                              "content": "Ø¨Ø­Ø«Øª ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆÙ„ÙƒÙ†ÙŠ Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ÙƒØŸ"})
            else:  # Path A: Default to troubleshooting the error
                # This is your existing, powerful troubleshooting logic
                pchain = create_followup_policy_chain()
                msgs = _last_msgs(2)
                raw = pchain.invoke({
                    "message_number": str(issue.get("message_number", "")),
                    "message_text": issue.get("message_text", ""),
                    "reason": issue.get("reason", ""),
                    "solution": issue.get("solution", ""),
                    "last_assistant_1": msgs["a1"], "last_assistant_2": msgs["a2"],
                    "last_user_1": msgs["u1"], "last_user_2": msgs["u2"],
                    "user_text": user_query
                })["text"]
                # ... (the rest of your original follow-up logic remains the same)
                try:
                    policy = _json_from_model_text(raw)
                except Exception:
                    policy = {"action": "ask_clarify", "assistant_reply": "Ù…Ø§ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªÙŠ ÙØ´Ù„Øª Ù„Ø¯ÙŠÙƒ ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ØŸ"}

                action = policy.get("action")
                reply = (policy.get("assistant_reply") or "").strip()

                if action == "escalate":
                    _escalate_and_reset(user_query)
                    return

                st.session_state.chat_history.append({"role": "assistant", "content": reply})
                st.session_state.memory.save_context({"question": user_query}, {"output": reply})

            if st.session_state.follow_up_attempts >= MAX_FOLLOW_UP_ATTEMPTS:
                _escalate_and_reset(user_query)
                return

            else:
                chain = create_troubleshooting_chain()
                reply = chain.predict(
                    message_number=str(issue.get("message_number", "")),
                    message_text=issue.get("message_text", ""),
                    reason=issue.get("reason", ""),
                    solution=issue.get("solution", ""),
                    chat_history=(st.session_state.memory.buffer if hasattr(st.session_state.memory, "buffer") else ""),
                    question=(user_query if step_no is None else f"Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ù…Ù‚ØµÙˆØ¯Ø© Ù‡ÙŠ Ø±Ù‚Ù… {step_no}. Ù…Ø§ Ø§Ù„ØªØ§Ù„ÙŠØŸ")
                ).strip()

            st.session_state.memory.save_context({"question": user_query}, {"output": reply})
            st.session_state.chat_history.append({"role": "assistant", "content": reply})

            if st.session_state.follow_up_attempts >= MAX_FOLLOW_UP_ATTEMPTS or "Ù†Ø­ØªØ§Ø¬ ØªØµØ¹ÙŠØ¯" in reply:
                _escalate_and_reset(user_query)
                return

            return st.rerun()

        st.session_state.chat_history.append({"role": "assistant", "content":
            "Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¶ÙŠØ­ Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù„Ù… ÙŠÙ†Ø¬Ø­ ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©ØŒ Ø£Ù… ØªØ±ÙŠØ¯ ÙØªØ­ Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø±Ù‚Ù… Ø±Ø³Ø§Ù„Ø© Ù…Ø®ØªÙ„ÙØŸ"})
        return st.rerun()

    # ===== 4) NEW-CASE SEARCH (no active issue) =====
    st.session_state.memory.save_context({"question": user_query}, {"output": ""})

    # Decide source first (so numbers inside how-to queries don't trigger false escalations)
    source = decide_source_smart(user_query)

    # If user explicitly asks to open new (no active issue), prompt for number/description
    if _simple_open_new(user_query):
        reply = "ØªÙ… ğŸ‘ â€” Ø£Ø±Ø³Ù„ Ø±Ù‚Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø£Ùˆ ÙˆØµÙÙ‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ Ù„Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©."
        st.session_state.chat_history.append({"role": "assistant", "content": reply})
        st.session_state.memory.save_context({"question": user_query}, {"output": reply})
        return st.rerun()

    # Errors-only: escalate if user typed a number that isn't in KB
    if source == "errors":
        valid_nums = st.session_state.get("valid_message_numbers", set()) or set()
        mentioned_nums = re.findall(r"\d+", user_query)
        if mentioned_nums and not any(n in valid_nums for n in mentioned_nums):
            st.session_state.chat_history.append({"role": "assistant",
                "content": "Ù„Ù… Ø£Ø¬Ø¯ Ø±Ù‚Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø°ÙŠ Ø°ÙƒØ±ØªÙÙ‡ ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„. Ø³Ø£ÙˆØµÙ„Ùƒ Ø¨Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ø¢Ù† Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø´ÙƒÙ„ Ø£Ø¯Ù‚."})
            _escalate_and_reset(user_query)
            return

    with st.spinner("ğŸ¤” Ø£ÙÙƒØ±..."):
        # Inside handle_text_query, within the `with st.spinner(...)` block:
        # Inside handle_text_query, within the `with st.spinner(...)` block:

        if source == "guide":
            res = llm_search_gl_guide(user_query)

            if "candidates" in res and res["candidates"]:
                if "candidates" in res and res["candidates"]:

                    st.session_state.rejection_count = 0
                    top_candidate = res["candidates"][0]
                    relevance_gate = create_relevance_gate_chain()


                    try:
                        snippet = (top_candidate.get("body") or "")[:400]
                        gate_raw = relevance_gate.invoke({
                            "query": user_query,
                            "title": top_candidate.get("title", ""),
                            "snippet": snippet
                        })["text"]

                        gate_result = _json_only(gate_raw)
                        is_relevant = gate_result.get("is_relevant", False)
                        confidence = gate_result.get("confidence", 0.0)

                        print(f"  -> Relevance Gate Check: Relevant={is_relevant}, Confidence={confidence:.2f}")

                        # Only proceed if the top result is deemed relevant with high confidence
                        if is_relevant and confidence >= 0.7:
                            st.session_state.pending_choices = res["candidates"]
                            st.session_state.pending_suggestion = None
                        else:
                            # If not relevant, provide a polite "I don't know" message
                            st.session_state.chat_history.append({"role": "assistant",
                                                                  "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ø±Ø­ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰ØŸ"})
                    except Exception as e:
                        print(f"Relevance Gate failed: {e}")
                        # Fallback to showing choices if the gate itself fails
                        st.session_state.pending_choices = res["candidates"]


                else:
                    st.session_state.chat_history.append(
                        {"role": "assistant", "content": res.get("answer", "Ø­Ø¯Ø« Ø®Ø·Ø£ Ù…Ø§.")})

                st.rerun()

                st.session_state.pending_choices = res["candidates"]
                st.session_state.pending_suggestion = None  # Ensure old state is clear
                st.rerun()
            else:
                st.session_state.chat_history.append({"role": "assistant", "content": res.get("answer", "Ø­Ø¯Ø« Ø®Ø·Ø£ Ù…Ø§.")})
                st.rerun()

        # The 'else:' block for errors remains untouched and will continue to correctly
        # set st.session_state.pending_suggestion when needed.
        else:

            # Errors KB path

            sr = search_gl_errors_llm(user_query)

            if isinstance(sr, dict) and "message_number" in sr:

                kb_title = sr.get("message_text") or ""

                mentioned_nums = re.findall(r"\d+", to_english_digits(user_query))

                title_like = (len(_normalize_for_title(user_query)) <= 64) or _looks_like_error_intent(user_query)

                ratio = _lexical_ratio(user_query, kb_title)

                # Keep your "did you mean?" rule: only if user didn't provide a number

                if title_like and ratio < 0.95 and not mentioned_nums:
                    st.session_state.pending_suggestion = sr

                    return st.rerun()

                # Accept and answer

                st.session_state.solution_provided = True

                st.session_state.follow_up_attempts = 0

                st.session_state.current_issue = sr

                st.session_state.current_issue_embedding = (

                    torch.tensor(sr.get("embedding", []), dtype=torch.float32)

                    if sr.get("embedding") else None

                )

                st.session_state.active_troubleshooting = True

                resp = generate_instructional_response(sr)

                st.session_state.chat_history.append({

                    "role": "assistant",

                    "content": resp.get("answer"),

                    "images": resp.get("images", [])

                })

                st.session_state.memory.save_context({"question": user_query}, {"output": resp.get("answer", "")})

                return st.rerun()

            # â— Nothing matched in errors KB

            if re.search(r"\d{3,}", to_english_digits(user_query)):
                # User explicitly gave a number â†’ escalate now

                st.session_state.chat_history.append({"role": "assistant",

                                                      "content": "Ù„Ù… Ø£Ø¬Ø¯ Ø±Ù‚Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø°ÙŠ Ø°ÙƒØ±ØªÙÙ‡ ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„. Ø³Ø£ÙˆØµÙ„Ùƒ Ø¨Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ø¢Ù† Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø´ÙƒÙ„ Ø£Ø¯Ù‚."})

                _escalate_and_reset(user_query)

                return

            # Otherwise fallback to conversational

            chain = create_conversational_chain()

            reply = chain.predict(question=user_query).strip()

            st.session_state.chat_history.append({"role": "assistant", "content": reply})

            return st.rerun()


def main():
    st.markdown("""
        <style>
            @keyframes gradient {
                0% { background-position: 0% 50%; }
                50% { background-position: 100% 50%; }
                100% { background-position: 0% 50%; }
            }
            body { direction: rtl !important; }
            .stApp {
                background: linear-gradient(-45deg, #FFFFFF, #F7F9FC, #F0F2F6, #F7F9FC);
                background-size: 400% 400%;
                animation: gradient 15s ease infinite;
            }
            [data-testid="stSidebar"] {
                background: rgba(255, 255, 255, 0.5) !important;
                backdrop-filter: blur(10px);
                border-right: 1px solid rgba(255, 255, 255, 0.2);
            }
            .block-container {
                max-width: 900px;
                padding-top: 2rem;
            }
            @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }
            [data-testid="stChatMessage"] {
                animation: fadeIn 0.5s ease-in-out;
                border-radius: 20px;
                box-shadow: 0 2px 4px 0 rgba(0,0,0,0.05);
                padding: 14px 20px;
                margin-bottom: 12px;
                width: fit-content;
                max-width: 80%;
            }
            [data-testid="stChatMessage"]:has(span[data-testid="chatAvatarIcon-assistant"]) {
                background-color: #FFFFFF;
                border: 1px solid #E5E7EB;
                text-align: right; 
                float: right;
            }
            div[data-testid="stChatMessage"]:has(span[data-testid="chatAvatarIcon-user"]) {
                background-color: #E6F2FF; 
                color: #1E1E1E;           
                float: left; 
            }
            .st-emotion-cache-1c7y2kd { width: 100%; }
        </style>
    """, unsafe_allow_html=True)
    initialize_session_state()
    render_sidebar()
    render_did_you_know_sidebar()

    if not st.session_state.get("kb_loaded"):
        render_welcome_screen()
    else:
        render_chat_interface()

if __name__ == '__main__':
    main()
